c----------------------------------------------------------------------
c     program nimrod:  Version (nimuw) 3_5_2, 3/23/20
c
c     NIMROD solves the time-dependent plasma fluid equations using
c     finite elements in the poloidal plane and Fourier components
c     in the periodic direction.  The equations are advanced in time
c     using a semi-implicit, time-split algorithm to avoid Courant 
c     restrictions due to waves.
c
c     The following vector fields are the dependent variables in the
c     equations:
c
c		be => magnetic induction
c		ja => total charge current density
c		ve => velocity [(momentum density)/(mass density)]
c		tion => ion temperature
c		tele => electron temperature
c		pres => total pressure
c		prese => electron pressure
c		nd => number density (ni=ne/Z)
c
c     The independent variables are:
c
c		t => time
c		r => radial coordinate
c		z => axial coordinate
c		x => logical coordinate #1
c		y => logical coordinate #2
c
c     Comment lines starting with c-TMP indicate a temporary or
c     exploratory change.  Those beginning with c-PRE indicate a
c     premature call or feature that has not yet been implemented in
c     the existing version.
c-----------------------------------------------------------------------
c-----------------------------------------------------------------------
c     code history:
c
c	0.0	Implementation of explicit, leap-frog Alfven waves
c		in proteus.  Mathematics and physics routines were
c		added.  In addition, provisions were made for 
c		non-Dirichlet boundary conditions.  This version has 
c		significant difficulty with nonuniform, orthogonal
c		grids. (CRS, 7/29/96)
c
c	0.1	A significant portion of the 2-fluid time-step advance
c		has been added.  The vector potential is advanced with
c		a distinct electric field.  Storage for the dependent 
c		variables is named logically, and generic storage has 
c		been dropped.  One may choose to keep a fixed magnetic
c		field, independent of the vector potential advance,
c		by specifying evolve_b=.false..  Note that if this is 
c		not done, the current implementation of the initial
c		conditions leaves the current density with a lot of 
c		noise (~1%).  Also, one now has a choice of leap-frog
c		or predictor/corrector [see input.f, where all of the
c		default values are now listed].
c
c		A rectangular grid may be made periodic in the 
c		y-direction by specifying pery=.true..  This allows
c		the magnetic field to be in the y-z plane (set
c		phib=0.5 and thetab>0.)
c		
c		The file rblock.f is again free of physics routines.
c		For each equation advanced by nimrod,
c		there is both a management routine and a physics 
c		integrand routine (see jfroma and pcov_ccurl, for 
c		example).  The integrand routine is passed to
c		rblock_get_rhs, which calls it during the
c		loop over gaussian quadrature points.  This is not
c		unrelated to the original proteus get_der and get_flux
c		routines, but it is more general in that the code
c		developer is not restricted to a flux/source form for
c		each equation.  
c
c		Performance on nonuniform grids has been 
c		improved through the use of a mass matrix that raises
c		the index on the lhs of the equations for J and B. 
c		This operator is consistent with the curl operator in 
c		the rhs.  Also the perpendicular directions of the
c		second-order derivatives appearing in the curl(curl())
c		operator may be lumped (specify linear=.true. and 
c		lumpperp=.true.).  This lumps the x-direction of 
c		d^2/dy^2 and the y-direction of d^2/dx^2, to be 
c		consistent with the lumped mass matrix on the lhs of 
c		the equation for J.	(CRS, 8/13/96)
c
c	0.2	This version has major changes in four separate areas:
c		time-advance, math, programming and gridding.  For the
c		time-advance, matrix solvers have been incorporated, so
c		that the dependent variables can be computed implicitly.
c		The user has three solvers to choose from at present.
c		When solver='bl_drect' in the input, nimrod calls Dan
c		Barnes' cg-solver, which uses a 2D direct solve within
c		each block as a preconditioner.  Setting solver=
c		'multi_gr' calls the same routines but specifies a 
c		preconditioner that does direct solves both within and
c		over blocks, in a multi-grid fashion.  Setting solver=
c		'diagonal' calls a separate Fortran 90 cg routine that
c		does only local, diagonal preconditioning.  One may
c		substitute other Fortran 77 cg routines at the it_solve
c		level (see iter_dd.f and Dan's documentation).  
c		New Fortran 90 solvers should be called directly from
c		the subprograms in nimrod.f--see calls to iter_diag.
c		Note that one may still run the code in an explicit 
c		manner by setting explicit=.true. and fom&npc as
c		described in input.f.
c
c		The index-raising mass matrices have been dropped in
c		favor of using scaling factors in the tensor components.
c		The scaled contravariant components have traditionally
c		been called 'physical' components, but we also use
c		related scaled covariant components.  See math_tran.f
c		and the reference therein for more details.
c
c		The rblock_type and edge_type&vertex_type definitions
c		have been placed in modules and files, which are 
c		separate from the rblock and edge subprograms, 
c		respectively.  The bulk of the data, namely the rb
c		and seam structures, is now collected
c		in a new module, fields, to make passing data easier.
c		Also, all integrand routines have been placed in
c		the module integrands, which constitutes file
c		integrands.f.  The rblock routine, rblock_make_matrix
c		is now similar to rblock_get_rhs in that it uses 
c		separate integrand routines.  Thus, all matrices are
c		created through rblock_make_matrix.  For an example,
c		examine esolve and how it creates the susceptibility
c		matrix.  Note that rb(:)%lmat is the work space for
c		creating new matrices, while rb(:)%dmat stores the
c		differential operator, which is time-independent.
c
c		This version can read a grid from the netcdf file,
c		nimrod.grid.  To use this option, set net_init=.true..
c		This sets the grid information in rb(:)%rz and the
c		block-connection information in the seam structure.
c		However, we do not have proper initial conditions for
c		such grids, and nonorthogonal coordinates have not
c		been tested, so don't expect much at this point.
c		CRS (9/20/96)

	0.4	[0.3 was 0.2 with the pre-processor]
		The most significant change from 0.2 is the incorporation
		of Tom Gianakon's nimset pre-processor.  Central to this
		change is restart writing and reading, and all relevant
		subprograms are placed in the module dump_mod in file
		dump_mod.f.  The fundamental dependent variables (A and V
		in this version) are initialized by nimset, along with
		the grid, and nimrod always starts from a restart.  Thus,
		nimrod can no longer start itself.  

                In addition, the boundary condition routines for 
		operators (dirichlet_op_t&n) have been changed.  The
		previous versions did not work on nonorthogonal grids.

		Finally, block communication has been changed to facilitate
		parallel coding.  There is no longer a communication step
		through an external boundary seam.  The boundary conditions
		to the rhs of an equation are now applied in a separate
		loop over blocks touching the external boundary.  There 
		is additional information in the block seams, including
		the ptr2 array, which is the connection pointers of ptr
		without references to seam(0).  The seam(0) is now split
		from the seam array [seam0] and is only used
		in initialization and in it_solve90.  Eventually, we
		expect seam0 and ptr to be deallocated after 
		initialization.
		CRS (10/22/96)

	1.0	This version of the code has three major changes from
		0.4:  1) 2-fluid physics, 2) toroidal geometry, and
		3) one Fourier component in the direction perpendicular
		to the finite element plane.  The 2-fluid physics required
		changes primarily in the susceptibility and jstar_2fl
		integrand routines for the electric field solve.  One may now
		choose between 2-fluid and single-fluid physics by setting
		numfluids to either 2 or 1, respectively.

		The toroidal geometry is incorporated through an additional
		item (#5) in the metric bicube structure.  When the input
		geom is 'tor', this item is equivalent to the cylindrical
		radius.  When geom is 'lin', it is set to unity, so that
		a periodic linear system may be modeled.  One may also choose
		to use a piecewise constant metric by setting met_spl
		to 'p_const' instead of 'cubic '.  Note that the covariant
		component of vectors is stored for the toroidal direction,
		but the Cartesian component is stored in linear goemetry.

		Limited variations perpendicular to the finite element 
		(poloidal) plane are now represented by a single Fourier
		mode.  Vectors now have six components representing
		(real_r,real_z,-imag_phi,imag_r,imag_z,real_phi) in toroidal
		and (real_x,real_y,-imag_z,imag_x,imag_y,real_z) in linear
		geometries.  This organization allows the same differential 
		operators to act on either the first or last three components
		only.  For now the single mode number is an input item, nz,
		and for linear geometry, one must also specify per_length,
		the periodic length of the system.
		CRS and RAN (11/6/96)

	1.1	Message passing has been added, so that NIMROD may be run
		on parallel architectures:

-----------------------------------------------------------

*** Modifications:

(1) minor changes bracketed with

c SJP start
...
c SJP end

Search *.f for any of these.  [Note--most SJP comments have been removed.
Only those which contain descriptive information and those which indicate
temporary changes remain.]

These are usually small tests for whether code is running in parallel
(nprocs > 1) or serial (nprocs = 1) and to take care of I/O.

Take special note of these minor changes:

** way that timers are called - code is now instrumented in several
places with calls to a "timer" routine - prints out timing statistics
at end of run - all the stored timers are in new module/file "time.f"

** added decompflag to nimrod.in and input.f

** moved .cray. variable from nimrod.f to system.f where other
system-dependent stuff is stored

** I changed the syntax of the call to wrdump to match that of rddump - 
makes more sense for parallel as well

(2) Subroutines with bigger changes, didn't mark these with "SJP", should
    be able to just replace the entire subroutine:

	edge_init - now only uses "seam0" for connectivity info
	rblock_create_tang - to avoid use of "ptr" and "seam0"
	rddump - only processor 0 reads file
	wrdump - only processor 0 writes file
	history - only a single processor writes file
	edge_network - enables parallel seaming

(3) New routines - not marked with SJP:

	ptr2_init - code taken from old "edge_init" routine
	timer - put it in mpi.f since MPI has a timer routine also
		could be in system.f - important thing is to isolate it
	deallocate_seam - in dump_mod.f - used to dealloc a temporary seam
	deallocate_block - in dump_mod.f - used to dealloc a temporary block
	history_init - in xdraw_out.f - sets up history-file writing
		so that only one processor (the one who owns the history pt)
		is doing it

(4) New files:

	makefile.t3e - new Makefile specific to T3E
	pardata.f - data structures to store parallel seaming info
	parallel.f - routines for parallel seam seaming and broadcast of
			info in nimrod.in
	parallel_io.f - routines to send blocks and seams from one
		proc to another - used by rddump and wrdump
	mpi_t3e.f - needed to hook F90 to MPI - need version for each machine
	mpi_serial.f - use when "making" serial code in place of MPI library
	time.f - where all accumulated timer values and timing routines
		are stored - except timer itself which is system-dependent
	system_t3e.f, system_c90.f and system_ibm.f, which contain system
		specific information for each platform including the 
		system module, the 'cray' parameter and the clock calls.

(5) Deleted stuff

	3 netcdf routines out of each of spline, bilinear, bicube - not used
	3 output routines out of each of bilinear, bicube - not used
	old timer.f - replaced with new time.f (time module) 
		and timer routines in the new system.f files.
	edge_alloc, surface_be, edge_send_extseam and edge_assign_be

(6) Stuff that is not yet done (for parallel):

	cannot call iter_90.f and iter_dd.f routines in parallel
	DX and PLOT routines are just skipped in parallel

-----------------------------------------------------------

	Also, there is a new input parameter, hist_binary.  If it is
	set to .false., the history data is written as formatted text.

		SJP and CRS (1/8/97)

	1.2	This version uses the bilinear magnetic field formulation,
		where electron and ion current densities are stored at
		the gaussian quadrature points.  See Alan Glasser's
		write-up on the magnetic field formulation for information
		about the implicit magnetic field equation.
		CRS and AHG (3/11/97)

	1.2.1	The bilinear B formulation has been revised and simplified
		to use a generalized Ohm's law instead of susceptibilities
		and transfer tensors.  There is a new input variable,
		ohms, where one can specify what terms are included.
		This change also involves using total current density (J)
		and momentum density (M) instead of species current densities.
		
		Also, the quadrature point storage of current densities
		has been eliminated.  One may specify either conforming
		or nonconforming approximations for curl(B).  For the
		former, J and M are bilinear, but for the latter, they
		are cell-centered piecewise constants.
		CRS (4/15/97)

	1.2.2	Blocks of unstructured triangular elements have been
		incorporated.  There is now a set of tblock routines
		to complement the rblock routines.  The integrand
		routines have been generalized, so they may be called
		from either the rblock or the tblock integration
		routines.  Thus, each physics equations only appears
		once.

		NIMSET has been modified extensively to produce polar
		grids with a 'pie' of triangles at the magnetic axis.
		such grids may be circular in shape, or they are 
		generated by flux surfaces from the output of an 
		equilibrium solver.  The new input variable, gridshape,
		determines what class of grid is generated.  In addition,
		when an equilibrium solution is read, it is used to
		initialize the 0-th order magnetic field.  Finally,
		there are new xdraw diagnostics.
		AHG and CRS (4/21/97)

	1.2.3	A divergence(B) cleaning routine has been added to the
		magnetic field solve.  It uses the operator grad(div())
		to diffuse errors.  Two input variables affect its
		operation.  The first is the diffusivity, divbd, and the
		second is the time centering fdivb.
		CRS (5/16/97)

	1.2.4	Two numerical options have been implemented.  The first
		is using a collection of degenerate rectangles instead
		of triangles at the center of a polar grid.  The grid
		will be constructed in this manner if triflag is set
		to F (otherwise, these cells will be the normal linearly
		splined triangles.)  The second option is the ability to
		use piecewise constants for grid information instead of
		cubic splines.  This makes the rectangles more consistent
		with the triangles, but it eliminates the smoothness
		provided by the cubic splines.
		AHG and CRS (6/3/97)

	1.2.5	Several important changes have been made to the physics
		kernel.  First, the advance of hall terms has been moved
		to a seperate semi-implicit time-split piece.  Previously
		within the b-field formulation, these terms have not been
		operational.  The factor controlling the magnitude of the
		semi-implicit term is now called si_fac_hall.  Second, an
		adiabatic pressure has been incorporated.  It also uses
		a semi-implicit scheme with a factor called si_fac_pres.
		The appropriate routine is called if the input variable,
		beta, is >0.  It presently sets a uniform equilibrium 
		pressure with respect to be0, and true finite beta equilibria 
		have not yet been implemented.  Finally, the divergence(b)
		cleaner has been moved to the end of the time step as
		an advance seperate from either the hall or mhd advances.
		CRS, RAN, DDS, AT (6/16/97)


	1.2.6	Minor upgrades: Alan has figured out how to get the assign
		command working on the crays.  This is now included in the
		open_bin and close_bin routines for the c90 and t3e, so
		that those machines write 'standard' binary files that may
		be moved to other machines.  This allows one to ship
		plot files to workstations for use with xdraw and dx, and
		it lets one port restart dumps.  The open_bin routine
		in the system_*.f files now have a 32/64 bit option for
		the assign call.  Binary files for plots use 32 bits, while
		dump files use 64 bits.  The latter now only contain
		64 bit reals and integers, so there are some conversions
		within the dump routines.  Also, the dump files no longer
		contain character variables.

		NOTE:  there is a compiler bug in the f90 binary writes
		on the c90.  We will be able to pack more data into
		each record (saving space) when this is fixed.

		ALSO NOTE:  the binary writes of 1D plots (nimhist.bin,
		jar*****.bin, jaz*****.bin, etc, and 1d.bin from fluxgrid)
		don't come out correctly unless everything being written
		is put into an array constructor.  This is probably
		related to the above note, though it would be preferrable
		to wait for a compiler fix before mucking with the dump
		routines at this point.

		Pressure plots have been added to the diagnose routines 
		and the xdraw_out routines.  Draw files in subdirectory 
		draw have been appropriately modified.  One more, an nxn 
		matrix solve is now used for the diagonal preconditioning, 
		so the 6x6 blocks are not broken into two 3x3s.  Another...
		(you thought this was from one coherent change) ...the
		use of the dmat array has been removed, since the split
		hall/mhd prevents saving one array, anyway.
		CRS, AHG, DDS, AT (6/20/97)

	1.3*	This was an attempt to used mixed/nonconforming elements
		with the electric field-based equations.  The electric
		field matrix was too ill-conditioned to be solved in
		practice, however.
		CRS (4/97)

	1.4	This is a semi-implicit version of the 1.2 series code.
		The momentum density advance has been moved before the
		magnetic field advance, and the B-field equation has
		a semi-implicit operator for mhd (as well as hall, though
		in separate split steps).  It is not clear that this
		will be our final choice in time-discretization, but it
		permits a faster route to toroidal mode coupling.

		This version includes updates so that parallel
		initialization is now complete for tblocks.  There
		is also new whistler wave initialization.

		The pressure advance has been modified, so that the 
		perturbed p can be represented by non-conforming elements,
		like b.  When lump_b is set to true, the dp/dt term in
		the p equation is lumped along with db/dt.  The b's used
		in J0Xb for the momentum advance now have the same 
		representation as the perturbed b (instead of assuming
		a conforming representation).
		CRS, SJP RAN (6/26/97)

	2.0	Nonlinear terms have been incorporated for JXB (force
		density and Hall effect), VXB, V.grad(P), and PV.  The
		pseudospectral operations occur entirely within 
		integrand routines.  The fft routine has been adapted
		from Zoran Mikic's fft2d and is most appropriate for cray
		vector machines.  These routines have been isolated in
		the new module, fft_mod.

		Further changes have been made to the input and mode
		array.  lmodes has been replaced by the more appropriate
		lphi (the number of cells in the phi direction is 2**lphi).
		The number of dealiased Fourier modes for nonlinear runs
		is nphi/3+1, starting from n=0.  Since lphi is not directly
		associated with the number of modes, linear runs have
		the mode array dimensioned by the new input, lin_nmodes,
		which is unrestricted.  Also, the old input, nz, has been
		completely removed.

		The dependent variable ve is now the single-fluid
		velocity instead of momentum density, and all components
		are cylindrical instead of having the phi-component
		contravariant.  Both changes have been made to facilitate
		a Laplacian viscous dissipation term, which has been
		added to the velocity advance.  There are now two
		advance_ve routines, one that uses viscous dissipation
		and no-slip boundary conditions, and the old one without
		dissipation but using free-slip conditions.  The
		order for solving components is different in the two
		cases, hence the need for separate routines.

		The two dirichlet_op routines (dirichlet_op_n and 
		dirichlet_op_t) have been replaced by the single
		routine dirichlet_op with the component specified as an
		passed parameter.  Besides 'normal' and 'tangent', it
		will also treat 'all' components for no-slip bc's for
		example.
		CRS (7/5/97)

	2.0.1	A predictor step for the MHD advance of B has been 
		added for nonlinear stability.  It is only used when
		the input parameter, nonlinear, is true.
		CRS (7/14/97)

	2.0.2	The equilibrium quantities (be_eq, ja_eq, pres_eq,
		and nd_eq) have been converted to the bicubic type.  They
		are splined in nimset, so values and derivatives are
		written to the dump files.  Fluxgrid and the analytic
		models for linear geometry must now provide the equilibrium
		J as well as B.

		A shape function for the electrical and viscous diffusion
		coefficients has been added.  The two new input parameters
		dvac and dexp control the shape.

		The DX writes in dxmod have been modified to write the
		real and imaginary part of b for each mode.  More control
		through input will be implemented soon.

		The 'inconsistency bug' has been fixed--input and output
		arrays to math calls were aliased.  Also, the cg solver
		now picks the best initial guess from 1) what is supplied,
		2) 0, and 3) applying the preconditioner to the rhs.
		CRS and AHG (8/14/97)	

	2.0.3	The iterative solver has been updated with a new
		preconditioning option, incomplete factorization.  It
		operates on individual rblocks (like the block-direct),
		so it still represents a form of domain decomposition.
		It is called when solver='bl_ilu_0' or 'bl_ilu_1' where
		the integer is the degree of fill-in.  In addition, off-
		diagonal elements along block boundaries are communicated
		with the new seam%segment structure.  This is effective for
		both incomplete factorization and full factorization.

		There was an unnecessary call to jfromb after the MHD
		bsolve when ohms='mhd'.  Advance has been modified to 
		eliminate it.

		The brhs subroutine in integrands has been split into
		two pieces, brhs_mhd and brhs_hall.  The functionality
		is unchanged (this is a time-split, semi-implicit 
		algorithm), but separating the terms reduces the number
		of IF-THEN blocks, so the code is now easier to read.
		CRS (9/5/97)

	2.0.4	The lmat array index order has been changed for 
		optimization.  For rblocks the index order is now
		lmat(i-vertex,j-vertex,comp_1,comp_2,i-offset,j-offset),
		and for tblocks, comp_1 and comp_2 have been moved lower
		in the structure to element,
		tb%lmat(i-vertex)%element(comp_1,comp_2,j-vertex).  This
		permits more array syntax and speeds component maniupulations,
		since the component indices have been moved forward.
		CRS (9/9/97)

	2.0.4.1 Tests proved 2.0.4 to be slower than 2.0.3 to my great
		surprise.  To go in the opposite direction of the previous
		change, component indices in rb%lmat have been moved to
		the end.  Thus, the rb matrix index order is:
		lmat(i-vertex,j-vertex,i-offset,j-offset,comp_1,comp_2).
		CRS (9/12/97)

	2.0.4.2 Profiling has been used to optimize iter_cg_f90.f and
		rblock.f.  Much of this has been incorporated into what
		is saved as version 2.0.4.1.  2.0.4.2 has the rb%lmat
		array back in the 2.0.4 order.
		CRS (9/17/97)

	2.0.5	The current density used on the right side of the B-field
		equations and the right side of the velocity update is
		now the local curl of the magnetic field, i.e. it's not
		a bilinear representation of J.  This eliminates the need
		for a mass-matrix solve when the Ohm's law does not
		include electron intertia.  In those cases, we will need
		gradients of J, so a bilinear representation will be
		needed; however, these terms are not yet implemented.  This
		change also make the J used on the left and right sides
		of the B-field equation more consistent (both are local
		curls of the new and old B, respectively).
		CRS (9/22/97)

	2.0.5.1	Parallel communication routines for edge segment-based
		have been incorporated into file parallel.f.   Other
		minor changes in dump.f, edge.f, nimrod.f, nimrod_init.f,
		pardata.f, time.f, and xdraw_out.f have been made for
		and to take better advantage of new information.
		SJP (9/25/97)

	2.0.5.2 Nimrod now reads the version number from the README file
		in subdirectory nimrod (if it can find it) and write this
		at the top of nimrod.out.  It also echoes the input
		namelists to nimrod.out.

		The block communication for degenerate rblocks has been
		changed for efficiency.  The points along the degenerate
		boundary are communicated internally, and only the
		corner points are connected.  The top left corner is
		a dummy (it's data is set to 0 before the inter-block
		communication) that must be left connected for the edge
		segment initialization.  The actual inter-block 
		communication is done with the lower left corners, which are
		all seamed together.  This keeps the communication work 
		from growing like my**2.  However, seam averaging had
		to be changed.  A new seam%vertex quantity, ave_factor,
		has the appropriate factor for each seam vertex, and
		the subroutine edge_div has been removed.
		CRS (9/30/97)

	2.0.5.3	The field-advancing management routines (bsolve, clean_divb,
		advance_ve, advance_visc_ve, and advance_p), have been
		modified so that the linear solve finds the new field
		instead of the change.  This puts the tolerance requirement
		on a more equal footing with other algorithms, and should
		reduce the number of cg iterations--particularly for a
		nonlinear run that goes to a steady state.
		
		Utility subprograms previously in file nimrod.f have been
		moved to a new file utilities.f.  This includes two new
		routines, nim_output and matvec.  The former checks the
		time step and calls appropriate plotting routines, and the
		latter manages a matrix-vector multiply.  Both have been
		added for convenience and clarity.
		CRS (10/10/97)

	2.0.6	New preconditioning options have been added to iter_cg_f90.f.
		They are based on 1D solves along grid lines.  The first set
		is line-Jacobi or line-diagonal, where the couplings along
		a grid line are inverted.  Specify solver='bl_diagx' or
		solver='bl_diagy' (or 'bl_diaga' for an average of the two
		directions) for these options.  The second set are
		based on consecutive 1D solves in each direction.  To get
		the x-direction solve first, specify 'bl_adi_x', and to get
		the y-direction first, specify 'bl_adi_y'.  These
		preconditioners are mainly intended for single-block problems on
		vector machines.  The single solves show a little improvement
		over diagonal preconditioning, but so far the alternating
		solves tend to impede convergence.

		Bicubic data structures now contain gs, gsx ,gsy ,gsxx ,gsyy,
		and gsxy arrays.  They are used for saving data at the
		quadrature point for efficiency.  Thus, bicube_getco and 
		bicube_eval routines are called during initialization and not
		during finite element integrations.  Only derivatives that are 
		used are saved, so all orders may not be saved.  If more
		quantities or derivatives are needed for new equations, make
		appropriate changes to bicube_save in file nimrod_init.f.

		The nimrod.out and history files are now opened and closed 
		every time they are used.  This forces a buffer flush, so that
		data will not be lost at a core dump or machine crash. 

		The dump file io has been included in the io timing, and
		there has been some cleaning up of node==0 checks and barrier
		calls.  In general, nimrod io is done by an individual node,
		and there must be fortran statements to produce this:

		1) There are some diagnostic routines that are only 
		called during single processor runs.  These include plot_x,
		dxmod, xy_slice, xt_slice, and yt_slice.  The calls to these
		routines and any associated initialization or finalization
		routines are within an nprocs==1 check before they are called.
		Thus, they reference no parallel routines or data.  They are
		timed, however.  There is also a check during initialization
		to see if any of these routines are requested when nprocs>1.
		If so, an error statement is issued, and the program is stopped.
		This makes the nprocs==1 checks around the calls redundant, but
		they still serve as warnings within the code.

		2) Other io routines are always called.  This includes nim_stat,
		nim_version, history, and dump_write.  They are timed and have
		mpi_barrier calls within the subroutine but outside of any
		node checking loop (as they must).

		3) There are strays. read_namelist is always called,
		but it is convenient to use the same input.f file for nimset.
		Thus, the node check is done in the main program.  Also it_unit
		writes are done directly from the management routines in
		nimrod.f.  There are no barrier calls or timings for them.
		_____

		The fft routines have been replaced with an F90 version of
		the routine in Numerical Recipies, 2nd ed.  The old version used
		reshape functions, which are extremely slow on the c90.  Now
		the FFTs usually only consume a couple of percent of the
		loop time.  The new packing routines do all of the packing
		at once, and handle multiple quantities for a vector field.
		Real arrays are returned with four indices--three for location
		and the last for component index.  Thus, there have been some
		minor changes to integrands.f

		The math_dot calls have been replaced with simple SUM functions,
		and math_dot has been removed from math_tran.  A new
		math_cart_cross3 has been added for use with configuration
		space arrays.
		(CRS and SJP, 10/29/97)

	2.0.7	Convergence studies of resistive tearing modes are showing that
		the code becomes numerical unstability before it loses accuracy
		as the time step is increased.  This is due to a
		reduction of the magnetic diffusion term by the semi-implicit
		operator.  To alleviate this, an option to have the diffusion
		time split from the rest of the magnetic advance has been
		incorporated.  Specify split_resist=.true.
		_____

		Momentum advection has been incorporated into the rhs routine
		for the velocity update for nonlinear simulations.  This
		is used when the new input variable, advect, is set to 'V only'
		or 'all'.  Future options will include current density
		advection in the generalized Ohm's law.

		When velocity advection is used, the velocity goes through a
		predictor/corrector step, and there is a CFL condition.  Thus,
		the time step is limited according to the new input v_cfl,
		dt=MIN(dtm,v_cfl*dx/V).  The code stops if dt drops to 
		dtm*dt_stop.
		_____

		The magnetic field used for computing the effective impedance
		tensor in curl_de in integrands.f now includes the n=0 mode
		for nonlinear runs.  This should be more accurate (and may
		be necessary for numerical stability) for cases where the
		n=0 mode becomes significant.
		_____

		A mechanism for having nimrod create its own equilibrium is
		now available.  One may specify a loop voltage, loop_volt,
		which is applied as a surface electric field in the rhs of
		the mhd step of bsolve.  This voltage may be ramped in time
		using tloopv0 and tloopv1.  Specify the vacuum toroidal field
		through be0 without generating current (for example, set
		lam0=0), then the equilibrium will only contain the vacuum
		toroidal field.  Apply a loop voltage in a nonlinear run
		and this will create the current and the rest of the
		equilibrium in the n=0 mode of the solution.  In addition to
		the surface E, the normal component of the n=0 mode is set
		to EXB/B**2.
		_____

		The computation of wavenumber associated with divergence(B) 
		has been moved from xdraw_out.f to utilities.f as a separate
		subroutine, divb_check.  This is done every time step, and
		if the result is > kdivb_2_limit, a new input item, the code
		stops gracefully.
		_____

		The energy in each mode is now computed (at nhist cycle-
		frequency), and the magnetic and kinetic energies are
		written to the binary file energy.bin.  The
		xdraw input file, drawen.in, has been placed in the draw
		directory as a sample.  The pattern is similar to drawxt.in for
		xt_slice.bin.   However, the space dimension is replaced by
		mode number.  Since only the n=0 pressure represents
		internal energy, the internal energy and total energy (sum
		of magnetic and kinetic over modes plus internal) is written
		to the end of nimhist.bin.  For nonlinear runs, the energy of
		the equilibrium field is included with the n=0 perturbation.

		The management routine for the energy computation is energies
		and is located in utilities.f, like divb_check.  The integrand
		routine is energy_density, and is located in a new module,
		diagnostic_ints in file diagnostic_ints.f.  The integrand for
		the divergence check, div_b, has been moved from integrands
		to diagnostic_ints.
		_____

		While implementing contour plots in nimplot, several bugs
		related to tblocks have been found.  1) the dump routines
		were not writing the block id's for tblocks, which are used
		in the parallel initialization routines.  2) matvec did not
		set the vector array before calling tblock_mvcmpy. 3) iter_init
		did not set block limits before inverting diagnonal elements.
		4) polar_seam_init (nimset) did not use the incremented rblock
		index number when setting block limits.
		(CRS, 11/11/97)

	2.0.8	There are a number of minor changes for polishing and in
		preparation for future changes.  First, the old plot_x coding
		has been removed.  Its function has been replaced by the
		xy_slice routines, which have more capabilities.  The probe
		histories (output to nimhist.bin) are now in a file called
		probe_hist.f, and the module is now named probe_mod.  Second,
		the makefiles for sgi and ibm computers now have a flag that
		declares r8 as the default kind for real variables and 
		constants.  This precludes a loss of accuracy through generic
		constants that appear throughout the code.  However, it also
		means that data written in 4 byte format for xdraw and dx must
		be passed through the command REAL(x,4) instead of just
		REAL(x).

		The third change is that the pressure used for computing the
		semi-implicit operator in p_si_op in integrands.f now includes
		the n=0 contribution for nonlinear runs.  This change is 
		related to the change in curl_de in version 2.0.7.  Fourth,
		bigr**2 occurred many place in integrands.f within the vertex
		loops.  The computation is now done before the loops and
		stored in a temporary variable.  Further optimization along
		this line seems possible.
		(CRS, 12/08/97)

	2.0.8.1	More io changes:  the do-loops over grid position and quantity
		index have been removed from the dump file write and read
		statements.  This puts each array in a single record, saving
		space and a little time.  The writes and reads include new
		field variables for future code development.  They are
		prese (electron pressure), nd (number density), conc (material
		concentration), ve_eq (equilibrium flow), and prese_eq
		(equilibrium electron pressure).  They have also been added to
		rblock_type_mod and tblock_type_mod.  To make the reads and
		writes more convenient, subroutines to read and write generic
		bicube, bilinear and tri_linear data structures have been
		implemented.

		New tests on the T3E show that when the -N option in the assign
		call from open_bin is removed, binary writes and reads are
		compatible with other machines.  This was made possible by the
		change in the xdraw writes described for 2.0.8.  This option is
		still needed on the c90, but it works without difficulty there.

		The new variables have been added to parallel_io.f for 
		communications before dump writes and after dump reads.  The
		routines have been streamlined with internal subroutines for
		each data structure type.

		Time histories have been changed to make better use of the
		xdraw data format.  The probe data is now written with
		different modes being separate records within a block of
		records.  Each time-set is then a separate record block
		(separated by a blank write).  This is similar to the energy
		data file, and the drawt.in file is now independent of the
		number of modes.  The file with the time-history source code
		has been changed back to 'history.f', and now includes the
		probe history source, the energy computations and writes
		(removed from utilities.f), and a new write of global values
		that are not functions of mode number to a file called
		discharge.bin.  Thus, there are now three time-history files,
		nimhist.bin (probe traces only), energy.bin (energy spectra),
		and discharge.bin (total energy, k**2 for div(b), etc.).
		xlog has been modified for the new file formats and can be
		used on all three files.

		The discharge history includes current, toroidal flux 
		(displaying the sum of _eq fields + n=0 and n=0 separately
		for nonlinear runs--only _eq for linear runs), and reversal
		and pinch parameters (based on _eq + n=0 for nonlinear or
		just _eq for linear).  Currents, fluxes, F and theta are
		computed in current_flux in history.f, with the new integrand
		routines, eq_i_phi and n0_i_phi, in diagnostic_ints.f.
		(CRS, 12/15/97)

	2.0.8.2	The changes here are mainly dealing with time-centering issues.
		First, the predictor/corrector loop in subroutine advance 
		has been commented out to avoid confusion and a potential
		source of error with the semi-implicit algorithm.  Similarly,
		the fom centering has been commented out.  The comment lines
		start with c-IMP for identification, since these features will
		be used in the implicit version of the code.

		The velocity prediction for nonlinear runs is now done with
		V.grad(V) and not the other forces.  This is more consistent
		with the B prediction.  A pressure prediction step has been
		added for the same numerical reason, and the centering on the
		corrector step is controlled by the new input variable fp_vdgp.
		Since the rhs integration routine for each needs to know
		for which step a calculation is intended, the character
		variable integrand_flag has been added to the global module.
		It takes the place of ohmswitch that was used for the b advance
		and is also used while advancing velocity and pressure.

		The dummy arrays passed into the tblock matrix vector multiply 
		routine, tblock_mvcmpy, have been changed to assumed-shape
		arrays.  This precludes an error on the sgi when checking array
		bounds and may results in faster code (as it did for rblocks).

		The input variable ilu_fac has been changed to off_diag_fac,
		and it may be applied to the bl_diag* and bl_adi_* options in
		addition to the bl_ilu_* preconditioners.
		(CRS, 12/17/97)

	2.0.9.1	Changes have been made (primarily in nimset and fluxgrid) in
		order to use tblocks in the region between the separatrix and
		the wall.  Instructions on how to do this are forthcoming.  It
		is also possible to use a triangulation at the center of the
		polar grid instead of the usual pie of triangles or degenerate
		rectangles.  Again, instructions will appear shortly.  However,
		for the necessary flexibility, the input triflag has been
		replaced by pieflag, which is a character variable.  To
		reproduce the former triflag=F option (degenerate rectangles)
		set pieflag='rblock'.  To reproduce the former triflag=T
		option (pie of triangles), set pieflag='tblock0'.  For help
		with the triangulation routines, contact Tom Gianakon.

		An io change not noted earlier is that the old xdraw plots
		across the grid have been removed.  Their functionality has
		been replaced by the slice plots.  Thus, the input variables,
		nplot, nrplot, jrplot, nzplot, and izplot have been removed.
		(CRS and TAG, 1/2/98)

	2.0.9.2	More io changes--hopefully this will settle down.  The slice
		routines in diagnose.f have been modified as follows:
		1)  In the xy_slice plots, all _eq quantities are written
		    first.  This allows one to use the same drawxyeq.in file
		    regardless of the number of modes written to the file.
		2)  The new equilibrium and dependent variables are written
		    to the files.
		3)  The _eq quantities are no longer written to the xt_ and
		    yt_slice files.

		Input:
		1)  the lin_nmax variable has been moved from the physics
		    namelist to the grid namelist.  
		2)  the solver_input namelist has been broken into two
		    namelists.  numerical_input contains most of the variables
		    (it's a more appropriate label), and solver_input now
		    contains only input related to the linear solver:  solver,
		    off_diag_fac, tol and maxit.
		(CRS, 1/2/98)

	****	If your old input file causes a crash while it is being read,
		check the following:

		1.  the logical triflag is replaced by character pieflag
		2.  ilu_fac is replaced by off_diag_fac
		3.  nplot, nrplot, jrplot, nzplot, and izplot are removed
		4.  lin_nmax is moved from physics_input to grid_input
		5.  solver, off_diag_fac, maxit, and tol are in solver_input,
		    and the rest of this namelist is now in a new namelist
		    called numerical_input

	2.0.9.3	Mainly bug fixes in nimset.  However, the nonconforming
		option has been disabled, since its present implementation can
		lead to memory allocation problems in tblocks (where the # of
		cells is > # of vertices), and it's rarely used.

		Also, the system files and modules have been changed 'local'
		to avoid conflict with the nearly-standard system utility.
		(TAG, AHG, CRS, 1/6/98)

	2.1	Several bugs related to tblocks and seam initialization have
		been fixed.  edge.f has been changed to create tangent vectors
		for tblocks as well as rblocks (block_create_tang called from
		edge_init).  The boundary.f file has also been modified so 
		that tblocks may be at the physical boundary of the domain.
		Both dirichlet_rhs and dirichlet_op have been modified to
		handle tblocks, and the routines are now in a boundary module.
		The history.f file has also been modified, so the F calculation
		will work with tblocks at the boundary.

		A bug in clean_divb affecting tblocks when geom==tor has been
		fixed.

		A new module, extrap_mod, has been added.  It contains a new
		structure, extrap_bl, and subprograms for fitting polynomials
		to old data, where the independent variable is time.  This is
		used to create initial guesses for the linear solver before it
		is called for each equation.  The order of the polynomial is
		set by the new input extrap_order.  When it is increased, the
		guesses are potentially more accurate at the expense of more
		memory and the CPU time needed for the extrapolation.

		A blunder in fft_mod.f has been fixed.  There should be no
		mpi call from any of the fft routines.  They are not needed for
		the timer.

		A new initialization routine, boundary_vals_init, has been
		added to nimrod_init.f.  It applies the appropriate boundary
		conditions to the initial B and V, so that errors are not
		carried forward in time--boundary conditions applied during
		the time step are only applied to the change, not the total
		solution.

		The magnetic field solves have been reorganized.  What was
		previously contained in bsolve and b_diffuse is now managed
		by adv_b_aniso, adv_b_iso, and bpredict.  The first is used
		for solving the mhd advance where the effective impedance 
		tensor is anisotropc (usual case).  The second is based on the
		old b_diffuse and is used whenever the effective imp tensor is
		isotropic.  This includes a resistive split (if requested),
		the present semi-implicit Hall time split, and the mhd split
		if mhd_si_iso=1.  The bpredict routine is only used for
		the nonlinear b prediction for the MHD split.  It is separated
		since it only involves a mass matrix solve, which can be
		managed more efficiently by itself.  Changes in integrands.f
		have also been made for this reorganization.  What was curl_de
		is now curl_de_aniso, and what was curl_detaj is now 
		curl_de_iso.  The former has been simplified, since it is only
		used for mhd, and the latter has been expanded, since it
		covers all equation where Z is isotropic.
		(TAG, SJP and CRS, 1/20/98)

	2_1_1	Version names now use underscores for compatability with
		cvs.

		So far changes are just bug fixes and cray problem avoidance.
		In nimrod, mx and my are computed from the grid itself, over-
		writing the input values.  This avoids potential
		inconsistencies with fluxgrid input.  mpi calls are made, so
		that this computation works in parallel.  A related computation
		in findvert in diagnose.f has been removed.  **Note that this
		assumes that rblocks are only in the polar region in the usual
		pattern.  It will need modification when rblocks are added to
		the rim.

		Routines for writing xt_slice, yt_slice, and time history
		files now place entire records within f90 array constructors
		to avoid a binary write error on the crays.  This error does
		not seem to occur with the xy_slice files--only files with
		time.

		A bug in prhs in integrands.f has been fixed.  The computation
		of grad(p) for the nonlinear v.grad(p) term did not include
		the factor of 1/R.  This only affected nonlinear finite beta
		runs, of which there have been few to this point.
		
		File existence checks have been added to dump.f, so that the
		code will not core dump if the file is not there.  [Similar
		checks have been added to nimset and fluxgrid, too.]
		(CRS, 2/13/98)

	2_1_2	The terms representing equilibrium flow have been incorporated
		into the rhs routines for velocity, magnetic field, and 
		pressure.  A few equilibrium flow profiles may be set with
		nimset via the new input variable eq_flow.  See input.f for
		more details.  We will eventually need to be able to read
		equilibrium flow profiles from equlibrium code output using
		fluxgrid.  This has not been implemented yet.

		In addition, the flow-cfl computation in subroutine new_dt
		in file utilities.f has been modified.  The effective cfl
		now takes the mass matrix and the Fourier series representation
		into account for the appropriate directions.  Thus, the
		computed time step limit for a given value of v_cfl will
		change when upgrading to this version of the code.

		The pressure term (Pe=Pi=.5*P) in Ohm's law has been added to
		the brhs_hall subroutine in integrands.f.  It only gets used
		when the input ohms='2fl'.
		(CRS, 2/23/98)

	2_1_3	A neoclassical closure based loosely on a linearized 
		Hirshman and Sigmar closure has been implemented. 
		The closure is nonzero only in rblock regions.
		To support this neoclassical closures the following additions
		were made:
		A new namelist neoclassical_input. 
		Two new routines in neoclassical.f: 
			advance_neo which advances the fluctuating anisotropic 
				pressure, 
			neo_init which initializes equilibrium quantites 
			associated with the closure, 
     		A new routine in integrands.f called neorhs.
		Modification of brhs_mhd to include an effective
		electric field into the Ohm's Law.
		A host of minor changes to initialize various variables.
		(TAG, 2/26/98)

	2_1_4	Several bugs have been fixed, and a couple of minor
		modifications have been made to the physics kernel:
		1) advance_visc_ve had a sign error when applying the matrix
		   to the old solution.  This would only affect toroidal cases
		   with coupling between r and phi components.  The fix seems
		   to have surprisingly little effect, however.
		2) The basis vector for the r-direction is not just r-hat in
		   the clean_divb routine.  Previously, this was not taken into
		   account in the boundary condition routines.  Again this only
		   affects toroidal simulations.
		3) adv_b_iso had a blunder when rearranging magnetic field
		   components for hall and isotropic mhd advances.  Simulations
		   with mhd_si_iso=1, which use adv_b_iso for the mhd instead
		   of adv_b_aniso, now run without generating large div(b),
		   and they seem to produce reasonable results.
		4) Nonlinear and advective predictor steps for b and p now use
		   all terms with semi-implicit operators instead of just doing
		   explicit predictions.  This is now consistent with the V
		   predictor/corrector advance.
		5) nimrod.f and integrands.f have been cleaned up a little,
		   removing commented-out lines that would pertain to an
		   implicit advance.  Averages after predictor steps have
		   been moved out of management routines and into advance
		   itself.
		6) Boundary conditions are applied to the B and V solution
		   vectors after each time split to avoid an accumulation
		   of error from the cg solves.
		7) iter_cg_f90 no longer uses rhs for doing edge networking.
		   The bbb array is not allocated.

		I take back the second sentense in 5).  The predictor steps
		used in the b, v, and p advances now store the 
		appropriately centered result in work1 DURING the
		mode loops in the management routines.  The B at the beginning
		of any time split is now in be until the corrector step is
		solved.  [The work2 array is no longer needed for saving B at
		the start of a split.]  Previously, the predicted variables
		were placed in the 'new' variable storage arrays.  For
		nonlinear runs, the first mode's solution overwrote the
		predicted value, so these predicted values--as used in the
		matrices for the different modes--were not consistent.

		This makes the management and integrand routines even more
		complicated, because there is more logic to storing and
		retrieving variables with the correct time centering.  However,
		a uniform pattern for management routines with predictor/
		corrector steps is now established:

		1. Extrapolate the old solutions to provide a guess for the
		   linear solver.
		2. Create the rhs for all modes.
		3. Start the mode loop:
		   A. Make the matrix that is applied to the CHANGE in the
		      variable.
		   B. Split components if possible for efficiency.
		   C. Find the matrix.(old solution) and add to rhs, so we
		      now have a matrix equation for the new variable (not
		      just the change).
		   D. Copy the extrapolated solution into the sln array.
		   E. Call the solver.
		   F. Copy the solution into work1, time-centered with the old
		      solution for a predictor step, or into the new variable
		      location for a corrector step.
		   End of mode loop.
		4. Re-apply bc's, to avoid accumulating cg error.
		5. Update the extrapolation routine's storage.

		Tom has also fixed a bug in the neoclassical initialization.
		(CRS, 3/13/98)

	2_1_5	The kinetic energy computation in diagnostic_int.f now 
		includes equilibrium flow if it is used and the case is 
		nonlinear.
		(CRS, 3/18/98)

	2_1_6	The most significant changes are intended to reduce execution
		time.  The extrapolation module has been revised so that
		the guesses used for corrector steps (for advection) are the
		result of the predictor step plus an extrapolated difference
		between predictor and corrector steps.  This brings corrector
		step iterations close to 0 during linear phases.  A new
		function, extrap_eval, is used to automate the guess
		computation, and extrap_update has been modified, too.  Note
		that when updating the predictor step extrapolation field, it's
		important to send the field at the end of the time step, not
		a centered interpolation.

		When implementing the corrector step modification, a bug in
		advance_visc_ve was fixed.  The phi-component index in the
		work1 interpolation from the predictor step was wrong.  This
		did not affect linear runs without equilibrium flow.  The V
		predictor step interpolations also had the dt factor in the
		wrong place, only affecting cases with fv_vdgv<1.

		The local_c90.f, local_t3e.f, and io.f files have been modified
		to use the full path name of files in open_bin and close_bin
		when calling assign.  This should prevent conflict when 
		running multiple nimrod jobs on the crays.

		Also, discharge_hist in history.f now puts out the natural
		log of internal and total energy plus a growth rate for the
		first Fourier component.
		(CRS, 4/6/98)

	2_1_7	New routines have been added and many existing routines have
		been modified to save matrices and their approximate factors
		(used for preconditioning the cg solve) to reduce computation
		time.  Here is a list of changes:

		1) A new module, matrix_mod, has been added which contains
		initialization and manipulation routines for the stored
		matrices.  A separate module, matrix_storage_mod, is presently
		located in the same matrix_mod.f file.

		2) Matrices are now created through the matrix_create routine,
		which calls rblock_make_matrix, tblock_make_matrix, and the
		boundary condition routine specified in the parameter list.
		matrix_create makes the matrix for all Fourier components, so
		it is called outside mode loops in the management routines.
		The management routines now point the generic lmat arrays
		to the appropriate stored Fourier component within each mode
		loop.

		3) The preconditioner factorization routine has been 
		separated from the cg solve in iter_cg_f90.f.  matrix_create
		calls the factorization routine (when a matrix is recomputed)
		after a generic factor pointer has been set to the permanent
		location for a given operator.

		4) Some of the manipulations done to reduce the computations
		for n=0 components became inappropriate in the new
		scheme.  While it is possible to use a limited set of the
		'nqty' vector components in a matrix, one can no longer shift
		indices, since that would overwrite permanent storage.  To
		avoid this, adv_b_iso, now computes all n=0 vector components
		simultaneously, though the phi component is uncoupled.  In
		addition, advance_visc_ve, uses separate matrices for the
		r-phi and z component solves, and it uses the fact that the
		n=0 phi matrix is the same as the n=0 r matrix.  These changes
		reduce the complexity of the management routines.

		5) Predictor and corrector matrices are now the same, and the
		semi-implicit operators use the 'old' fields, instead of a
		time-centered prediction in the corrector step.  This shouldn't
		have much impact, but it needs verification.

		6) The dirichlet_op routine has been modified so that it
		may be called for a limited set of vector components.  This
		is now required to avoid problems similar to 4) above, and it
		has been used for the velocity matrices.

		7) The unused b_predict, 'vdgv only', and 'vdgp only'
		predictor step options have been removed.  They have not been
		used since 2_1_4, and they would require separate predictor
		matrices.

		8) There are a couple of new input parameters to help limit
		the frequency of matrix/factor recomputes in nonlinear runs.
		Matrices are recomputed whenever the time step changes, and
		semi-implicit operators are recomputed when the average field
		(equilibrium + n=0) changes by some specified amount.  The
		first parameter is dt_change_frac.  If cfl conditions tend to
		limit dt, it will be decreased by a factor of dt_change_frac,
		so that a gradual cfl change will not force a recompute every
		cycle.  If the cfl conditions allow larger dt, changes will be
		held until a dt_change_frac fraction is allowed (unless the
		time step has not changed for the last 10 cycles, in which
		case, a smaller change is allowed).  The limit on how much the
		average fields can changes before new semi-implicit operators
		are computed is set by ave_change_limit.  An additional input
		is dt_incr, which limits how much the time step can increase
		from one cycle to the next.

		It should be noted that this version of the code is somewhat
		temporary, since the matrix storage modifications suggest
		other ways to improve the data structure of the code.  This
		will lead to cleaner code with fewer separate loops for
		the different types of blocks.
		(CRS, 4/13/98)

 2_1_8

  This revision embodies a fairly extensive reorganization of data and
the way it is handled in different aspects of the code.  It is
intended to make the code more modular with fewer instances of
repetitive fortran.  The management routines for organizing finite
element solves of physics equations have been streamlined (nimrod.f is
down by more than 500 lines), and new management routines should be
easier to write.  In addition, routines dealing with one aspect of the
time advance no longer have access to data associated with other
aspects.  The first part of this description lists new files and
modules.  The second part gives more details on specific changes, and
the third part compares data storage and usage in different parts of
the code.  The fourth part is another modification to improve coding
but independent of the data reorganization.

I. NEW MODULES/FILES:

vector_type_mod / vector_type_mod.f in the nimset directory
 -> defines a global structure that holds vector data at all nodes

seam_storage_mod / seam_storage.f in nimset
 -> holds the seam array, seam0, and exblock_list, formerly part of fields

matrix_type_mod / matrix_type_mod.f in the nimrod directory
 -> contains definitions for matrices and matrix factor structures

matrix_storage_mod / matrix_storage.f in nimrod
 -> stores structures for matrices and their factors

matrix_mod / matrix_mod.f in nimrod
 -> contains routines to perform matrix/vector multiplications and routines
    to lump operators

computation_pointers / computation_pointers.f in nimrod
 -> contains the generic rhs, sln, and cell_rhs vectors that are used in the
    matrix equations resulting from the finite element discretization

finite_element / finite_element.f in nimrod
 -> contains the generic matrix_create and get_rhs that call the appropriate
    block-based finite-element integration routines

II. DESCRIPTION OF SPECIFIC MODIFICATIONS:

  In general, the advance of each physics equation solved by NIMROD
can be broken into the following aspects: 1) finite element
integrations, 2) vector manipulations, 3) a linear system solve, and
4) networking data across grid blocks.  While 4) plays a role in 1)
and 3), the operations can be considered distinct.  The management
routines in the nimrod.f file organize the different aspects, and
their sequential application constitute a time step.
  The purpose of the data reorganization is use the separation of the
aspects to enhance modularity and readability, and to make it easier
to add new physics to the code.  All spatially-dependent arrays were
contained in the rb and tb block structures, which were stored in the
fields module along with the seam structures.  This was inconvenient
for the vector and matrix manipulations.  It forced us to have
separate rblock and tblock loops in the management routines for vector
manipulations.  It also prevented us from passing matrices to the
iterative solver and matrix/vector multiplication routines, since they
were in structures that had a lot of other data.  Furthermore, edge
networking routines had complete access to all data, though they only
required the seam structures.
  The rb and tb structures are appropriate places for the
finite-element functions of space (the physical fields), which are
used during integrations.  The matrix, rhs, and sln arrays have been
moved from these structures, since they serve separate purposes.
Different aspects meet at the output end of the finite element
integration routines, (r/t)block_make_matrix and (r/t)block_get_rhs,
and now the block and matrix structures are passed separately into
these routines.
  To streamline the management routines, the finite_element module has
been added.  It contains the matrix_create routine from 2_1_7's
matrix_mod, and the new get_rhs routine.  They take care of the
separate block loops which call block-specific integration routines.
In addition, they call a boundary condition routine specified in the
parameter list (like the specified integrand routine) and the edge
networking routine where appropriate.
  Vector pointer structures (defined in vector_type_mod) have been
added to fields.  They are 2-level structures with the field (be, for
example) at the upper level as a block array, and a 3d array at the
lower level.  The array is a permanent pointer associated with the
appopriate nodal information, e.g., be(ibl)%arr=>rb(ibl)%be%fs, for
1<=ibl<=nrbl.  [Alan has already done something like this in regrid.]
While the rb/tb form of the data is appropriate for the finite element
integrations, only the node vectors are used during vector
manipulations in the management routines.  Since the form is the same
for both block types, vector manipulations no longer need separate
rblock and tblock loops.
  [In addition, the dummy (middle) index for tblock arrays has been
changed to 0 from 1 for consistency with rblock arrays.  Old dump
files with tblocks can not be used with the new code, since the seam
intxy pointer stores this dummy index.]
  Rounding out the change to fields, seam and seam0 have been moved to
the new seam_storage module.  This change has only been done to make
the organization consistent, and it doesn't have more serious
implications.
  Turning to the linear solve, matrices and factors are now passed as
structures into iter_solve and iter_factor.  Thus, permanently stored
matrices are passed directly without a separate pointer assignment.
The rhs and sln vectors are passed as vector_type structures.  They
are stored in the computation_pointers module.  This is only a matter
of convenience, since they could now be treated as local structures
within each management routine.  The arrays in the rhs structure are
also passed into the rhs finite element routines in rblock and tblock.
  The matrix/vector multiplication routine, matvec, resides in
matrix_mod and is used for both the linear solve and for vector
manipulations in the management routines.  It is more general than the
old version which resided in utilities.f, since matrix, operand and
product structures are passed.  The rblock- and tblock-specific coding
have also been moved into matrix_mod (from rblock and tblock), since
they did not influence the finite element computations, to which those
modules are devoted.  To make a clean break from the finite element
data, the tblock matrix structures now keep their own copy of the
vertex neighbor array (a small price).

III. DATA STORAGE AND USE COMPARISON:

CODE ASPECT	DEFINITION MODULES	STORAGE MODULES		USE MODULES

f. e.		rblock_type_mod		fields			rblock
computations	tblock_type_mod					tblock
								integrands
								diagnostic_ints

vector		vector_type_mod		computation_pointers	mgt. routines
manipulations				fields (via the new	finite_element
					   vector pointers)

matrix		matrix_type_mod		matrix_storage_mod	matrix_mod
manipulations							iter_cg_f90
								finite_element

networking	edge_type_mod		seam_storage_mod	edge


  A final note regarding the use of pointers in this version:  in
version 2_1_7, pointers were used to point to stored structures (such
as matrices and factors), and routines from different aspects had
access to these pointers.  With the exception of the new pointers in
fields, this use of pointers has been replaced by passing structures.
This helps increase modularity.  However, pointers have been used in a
local fashion within block loops in iter_cg_f90, for example, to point
to the part of the structure associated with a particular block.
Writing out the entire structure seems to inhibit optimization on our
SGI Octane, and the use of pointers prevents an increase in
computation time.  This doesn't seem to hurt on the c90, but there is
a small penalty with respect to 2_1_7 on the T3E (which may be due to
the pointers, but that's not clear).  Using pointers in these
situations also makes the code much more readable.

IV. NEW GENERIC_EVALS MODULE

  Part of the bulk of the integrand routines is related to evaluating
functions, derivatives, and test functions that are used for each
integration.  This has been compounded by having separate code for
rblocks and tblocks.  The new generic_evals module helps alleviate
this.  It contains a generic_all_eval routine which calls the
appropriate routine for evaluating a function and its derivatives
regardless of its type, i.e. bicubic, bilinear, or tri_linear.  The
structure of the integration and integrand routines forces us to pass
a real and a dummy block into the integrand routine (one of each
type), and this pattern is repeated in generic_all_eval.  [We cannot
pass a block into a generic dummy block in the integrand routine, so
this is the best compromise.]  Another new routine is
generic_alpha_eval which fills the arrays for the test function and
its derivatives.  With these changess, there is no more
block-specific coding in the integrand routines.  They also reduce
integrands.f by 650 lines and diagnostic_ints.f by 80 lines.

V. CHANGES IN THE DIV(B) CLEANER

  The divergence cleaner has been modified so that the finite element
representation of b now uses the same basis vectors as elsewhere in
the code, namely alpha*r_hat, alpha*z_hat, and alpha*phi_hat/r, where
alpha is the 2D finite element.  The changes affect clean_divb in
nimrod.f and grad_div and divb_rhs in integrands.f.

  (CRS, 4/27/98)
								

  2_1_9

  The extrapolation routines have been fixed to handle nonlinear cases
without advection, which do not have a predictor step in velocity.
There was a bug such that a predictor step was always expected for
nonlinear cases.  The per_block function in iter_cg_f90.f has also
been fixed to correctly check processor number as well as local block
number.
  With the divergence cleaning using the same basis vectors as the
other magnetic advance equations, it became possible to clean
simultaneously within the other time splits.  A new input logical,
split_divb, controls this.  The default value is true for consistency
with the old approach.  If it is set to false, divergence cleaning is
done in BOTH the mhd and Hall time splits.
  CFL computations based on linear and nonlinear wave speeds have been
added to the new_dt routine.  In addition, nl_cfl_lim is a new input
variable that allows one to set a nonlinear cfl limit.  Its default
value is very large, so this variable must be set to impose a limit.
  Another new input variable is ds_use in the physics namelist.  It
determines whether diff_shape is applied to both elecd and kin_visc,
just one, or neither of them.
  The extrapolation for time-split divb cleaning is treated like a
corrector step, i.e., the change from the previous step is
extrapolated.  This seems to save a few cg iterations in some cases.
  The block-boundary averaging in iter_cg_f90.f has been converted to
a symmetric operation.  Now, preconditioners other than diagonal
converge with multiple blocks.  In addition, the line diagonal (line
Jacobi) preconditioner has been modified to do an automatic search for
the off-diagonal weighting along each line.  Here, off_diag_fac is now
used as a lower limit.

  CRS, 5/29/98


  2_1_10

  Collecting off-diagonal matrix elements that touch degenerate points
so that connections in the rblock pattern are only in the x-direction,
seems to help the preconditioning.  The modification is made in
matrix_create in finite_element.f, though the matrix represented is
unchanged (to roundoff).
  A singularity check is now done in iter_solve_sym (ilu
factorization), similar to what was done for math_solve_sym in version
2_1_9.  If the pivot block is singular during an ilu factorization,
nim_stop is called to prevent a core dump.  Also, the adi
preconditioning option has been removed, since it has never worked.
  Bugs in integrands.f and utilities.f have been fixed.  In vrhs,
inadvertently setting advection to something other than none in linear
runs without equilibrium flow led to an access of work1, which was
undefined in those cases.  In new_dt, there was a division by nl_cfl
even when it was 0.
  Some optimization has been done in matrix_mod.f in routine
matvec_rbl, in iter_cg_f90.f for diagonal preconditioning, and in
fft_mod.f.  Unfortunately, this only seems to help performance on the
SGI Octane and not on the Cray vector machines or T3E.

  CRS, 6/19/98


 Changes incorporated from the v_si2 branch:

  Version v_si2 was offshoot from 2_1_9, where an option to use a
semi-implicit operator in the velocity equation instead of the
operators in the b and p equations was added.  Set the input si_in_v=T
to make the switch.  There is also an option to split the mhd and
viscous+advection terms in the velocity advance.  Set split_visc=T for
this.

  CRS, 6/11/98

  An anisotropic semi-implicit operator for the velocity equation has
been added.  It is based on substituting an ideal magnetic field
change into the perturbed j in the Lorentz force (then integrating by
parts).  The derivation is done assuming uniform density.  If density
is not uniform, there is a term missing, though it may be better to
return to having momentum density instead of velocity as a Galerkin
quantity.  As with the si operator for b, one can get a mix of
isotropic and anisotropic operators by specifying the mhd_si_iso
parameter between 0 and 1.  The new management routine, adv_v_aniso,
has been added to nimrod.f, and the new integrand routine is
v_aniso_op.

  CRS, 6/12/98

 End of changes from v_si2 branch.


  2_1_11

  Besides the changes for the option to have the MHD semi-implicit
operator in the velocity equation, the most substantial change allows
different groups of Fourier components to reside on different
processor 'layers' on parallel machines.  MPI is still used for all
communication, but much has been added to parallel.f and parallel_io.f
to separate Fourier communication on each grid-block and layer
communication for a set of Fourier components.  For example, the
communication that occurs during matrix-vector multiplies is layer
communication, since our matrices do not couple different Fourier
components.  This allows us to solve the different matrix equations
for different F-comps simultaneously on the different processor
layers.  Another example is the communication done for an FFT, which
is F-comp communication only; different grid blocks do not need to
communicate for this.
  Several other files have been modified to allow the layer
decomposition.  These include dump.f (to distribute and gather layers
at read and write times, respectivesly), fft_mod.f, and utilities.f.
Note that more care must now be exercised to find the n=0 Fourier
component used for matrices, since it may not be part of a layer's set
of components.  Several other files have minor modifications, so that
the appropriate set of nodes is used in mpi_barrier calls.
  The fft_mod module has been changed back to using the old fortran
fft from Zoran Mikic.  It is substantially faster than the Numerical
Recipe-based version on the T3E and SGI Octane.  The speed of the fft
becomes more important as the number of layers is increased.
  A new IF-block has been added to nim_stop for parallel cases.  This
calls mpi_abort if the message indicates that the termination is not a
normal one.  There are many places where nim_stop may be called by a
single Fourier component and/or a single block so that one processor
may call nim_stop but not the others.  The mpi_abort call forces all
processors to stop.  Though it is less than perfectly graceful, this
prevents the processors which don't call nim_stop from sitting idle at
a barrier, eating up CPU time.
  The ave_field_check routine in utilities.f has been modified.  It
now correctly communicates ave_b_change and ave_p_change to all
processors to prevent the situation where some expected to create
a new matrix, while others did not.  This led to hangs in parallel
nonlinear runs.
  The management routines in nimrod.f have been modified to perform
the inverse fft calls at the grid vertices.  This allows us to
interpolate functions of configuration space to the quadrature points,
instead of calling the inverse ffts at all of the quadrature points.
The forward ffts are still performed at the quadrature points after
pseudospectral operations, however.

  SJP and CRS, 8/10/98

  pre_2_2

  The inverse ffts at the grid vertices led to very ugly code, and the
logical extension of moving the forward ffts was even worse.  The
changes have been removed, putting the onus on getting the ffts and
pseudospectral operations to scale with processor-layer number.

  CRS, 8/12/98

  2_2

  The ffts and configuration-space operations called from the
integrand routines have been made to scale with nlayers by breaking up
the poloidal dimensions within each block.  The ffts now use an
all-to-all matrix type communication, and some of the data shuffling
in the fft wrappers have been streamlined for nimrod.
Configuration-space arrays in the integrand routines are now
3-dimensional arrays, where the first index is a (single) poloidal
cell index, the second is the toroidal position, and the third is the
vector component index.  The two poloidal dimensions in an rblock are
now represented by the first poloidal index, but be aware that any one
layer does not have access to all configuration-space information
across a block.  This does not matter in the integrands we now have,
since all operations (dot or cross products) are just local to each
point.
  The parameters passed to fft_nim have been modified.  The two
parameters specifying the non-transformed poloidal dimensions are now
the total dimension of the Fourier coefficients (previous nx *
previous ny), and the total dimension of the poloidal dimensions of
the configuration-space data.  The appropriate dimension for the
latter for pseudospectral operations is stored in the mps_block array
in global.  For each block, mpseudo (also in global) is set to the
correct value in get_rhs in finite_element.f.  fft_nim knows how to
sort out what goes where if the config-space array is allocated with
either this dimension or the dimension of the entire block (for
diagnostics or cfl computations, not pseudospectral operations).

  CRS and SJP, 8/14/98

  The ilu factorization now has a loop, within the block loop, to test
different values of the off-diagonal factor.  This is similar to what
is done in iter_line_fac, but here running into a singular submatrix
affects the factorization for the entire block.  As with the bl_diag*
options, off_diag_fac is now used as a lower limit for the values
tested.
  A new control parameter, cpu_tmax, has been added the numerical
namelist input.  This allows one to set an approximate maximum cpu
time, which is helpful for batch queues with cpu time limits.  For
parallel processing, the time is computed on each processor (not a
total time).
  The default values for dt_change_frac and ave_change_limit have been
reduced to 0.1 and 0.01, based on experience with RFP computations.

  CRS, 8/17/98

  The reversal parameter computation has been changed so that it
limits to unity for a vacuum field in an arbitrary cross section in
toroidal geometry.  It is now 
mu0*(net poloidal shell current)*integral(dA/r)/(2*pi*flux).

  CRS, 8/19/98

  One may now set a desired amount of net current for the n=0 part of
the solution (not including plasma current represented in the
equilibrium fields) through the new input i_desired.  The loop voltage
will then change according to the rate constant, loop_rate, in order
to get the current to relax to the desired amount.
  The applied loop voltage is now written to the dischage.bin file for
assessing the impact of the new option.
  Another new input, dt_initial, sets the initial time step when it is
nonzero.  This can be helpful when restarting nonlinear runs.
  The time step release (allowing dt to increase by an amount less
than dt_change_frac after some number of steps) has been circumvented.
Simulations running near the flow cfl condition seem to have slightly
larger velocities as dt is increased.  This leads to a situation where
the time step may oscillate.  Killing the release is part of the
solution.  I would also recommend setting dt_incr to one plus
f*dt_change_frac, where 0.5<f<1.  The default dt_incr has been changed
to be consistent with this and the default dt_change_frac.

  CRS, 8/21/98

  The time step release is back in.  The previous arrangement did not
allow the time step to increase (ever) when dt_incr <
1+dt_change_frac.  Now the release is activitated after 10 steps at
the same dt, as long as the new dt isn't back to the flow cfl limit,
which prevents the dt oscillations.

  CRS, 8/27/98

  A new input parameter, lump_all, lumps the mass matrix in all of the
time advance equations.  This seems to be helpful for preventing
cell-to-cell oscillations in cases with unstable equilibria.
  The anisotropic semi-implicit operator in the velocity equation has
been modified so the operator that stabilizes sound waves has the
form, -grad(cfl**2*div()), before integrating by parts.  Previously,
the Laplacian was used.  One may get a mix of the grad-div and
Laplacian operators through the mhd_si_iso parameter, which also still
affects the mix of the anisotropic/isotropic operators for the ideal
mhd terms, i.e., as mhd_si_iso=>1, both parts limit to the isotropic
Laplacian.
  A bug in the wave cfl computation has been fixed.  I forgot to
change the phi-component of B to cylindrical before finding B**2.
This only affects toroidal simulations.

  CRS, 9/2/98
  
  The separate electron pressure advance has been added (adiabatic for
now, like the total pressure advance).  To advance the electron
pressure as a separate equation, set separate_pe=T in the input file.
The equilibrium and initial perturbed electron pressures are set in
nimset as pe_frac times the total equilibrium and total perturbed
pressures, respectively.  If separate_pe=F, the electron pressure is
pe_frac*(total pressure) for all time.

  AT and CRS, 9/2/98

  Terms representing J0xb_tilde and v_tilde.grad(P0) have been added
to the semi-implicit operator in the velocity equation.  Thus, this
operator now has the form of the ideal linear MHD potential energy.
This allows running at time steps larger than the Alfven time, where
previously, fast growing but spatially smooth numerical modes
developed.
  A new input parameter si_fac_j0 multiplies the terms.  While they
are semi-implicit terms that have been symmetrized, I don't recommend
setting si_fac_j0>1.  The terms may not be positive definite.  The
default is si_fac_j0=1.

  CRS and AHG, 9/23/98

  Parallel and serial seam communications have been modified so that
summations will occur in a unique order on all sides of a border.
This prevents a problem with numerical garbage accumulating at block
corners over many time steps.  In addition, parallel computations
should now produce exactly reproducable numerical results.
  The T3E timer has been changed to the library SECOND routine to
return elapsed time.  The wall clock timer, mpi_wtime, led to problems
when batch runs were stopped for system maintenance then restarted.

  CRS and SJP, 9/26/98


  2_2_1

  The loop voltage control to maintain constant current now allows a
nonlinear response (see input.f) to help reduce over-shooting without
invoking dI/dt terms.
  The makefile_IRIX64 file has been modified to allow the option of
producing a parallel executable from a locally available mpi library.

  CRS 10/27/98


  2_2_2

  The nonlinear current response has been removed from the
loop_voltage feedback, and a dI/dt term has been added.  Use
loop_rate2 (units of Ohms) to control the dI/dt feedback.  Note that
old I is not saved from run to run, so inserting a restart in a time
sequence will change results slightly.

  CRS 11/5/98


  2_2_3

  The rblock matrix-vector multiply has been optimized for the T3E.
To make better use of the cache, the grid offset indices have moved to
the first array indices.  Therefore, any subroutine that works with
rblock matrices had to be modified.  This includes routines for the
iterative solver, boundary conditions, finite element computations,
and matrix manipulations.
  The 'old' in fft_mod.f has been converted to use a complex work
array for optimization, but the improvement is slight.

  CRS 12/11/98


  2_2_4

  The nimr0 branch is being merged into the main line at this point.
There is a new module, regularity, which handles the regularity
conditions at any R=0 vertices in an analogous manner to the way
boundary conditions are applied.  The nodal quantities for the phi
components of B and J are converted to cylindrical; however,
equilibrium B and J phi components remain covariant and contravariant,
respectively.  The only new input related to the regularity conditions
is r0dr_weight.  In the finite element formalism, the Neumann-type
regularity conditions for the n=1 Fourier component are not enforced
via 'natural' boundary conditions, since the area of the 'surface' at
the axis is zero.  Thus, a constraint integral, 

  r0dr_weight*integral( eta(position) * r**2 * (d( )/dr)**2 )

is added to the matrices for the change in the r and phi n=1
components of the solution vector.  Here eta() is a bilinear (linear)
spatial function in rectangular (triangular) cells with all vertex
values set to 0, except at r=0 vertices.  At the r=0 vertices, values
are normalized to the average of the r=0 diagonal elements of the
matrix to which the constraint integral is added.  The default value
of the coefficient r0dr_weight is unity, but it may be changed in a
particular simulation if the Neumann conditions appear to be grossly
violated.
  
  An unrelated development is an isotropic semi-implicit operator
designed to handle nonlinear magnetic and internal pressures.  It is
used in the velocity equation when si_in_v is true.  This should
remove the guess-work of fiddling with si_fac_mhd, si_fac_pres, and
mhd_si_iso in nonlinear simulations.  Those coefficients can be used
for the part of the semi-implicit operator intended to handle linear
(in n) forces, while the new isotropic part automatically adjusts to
the local nonlinear pressures.  Setting si_fac_nl (the new
coefficient) to a number that is a little larger than unity should
provide stability in nonlinear simulations.

  CRS, 12/11/98


  2_2_5

  More changes have been made for optimization:

1) Complex matrices have been introduced to take advantage of faster
complex algebra in matrix-vector multiplications and cg operations.
There are now two copies of routines that create and handle
matrice--real and complex versions.  Higher-level routines like
matrix_create use f90 interface statements, so that the same routine
name appears when the operation is called for either a real matrix or
a complex matrix.  The two different cg versions are in separate
modules, iter_cg_real and iter_cg_comp, linked through the generic
iter_cg module.  The complex module is in file iter_cg_comp.f, while
the real module remains iter_cg_f90.f; the switchyard module is
iter_cg.f.
  Duplicate matrix routines have also been made for boundary,
regularity, rblock, tblock, and matrix-vector operations.  The
integrand routine for v_aniso_op has been converted to a complex
integrand.  Both adv_v_aniso and adv_b_aniso now use complex matrices.
  There has also been a bug fix in the divb cleaning operator when the
cleaning is used unsplit from an si operator in B.
  
2) The ilu factorization no longer uses the coupling to the degenerate
point if there are multiple blocks around that point (nybl>1).  It has
been found that using the coupling in the ilu impedes convergence in
these cases.

3) The configuration space operations in new_dt have been modified to
work in parallel when nlayers>1.

4) Calls to mpi_barrier have been commented-out in edge.f,
utilities.f, history.f, iter_cg_*.f.

Aside:  the fft calls in neoclassical.f have been updated for the
multiple-layer syntax (should have been done a long time ago).

  All of this saves about 30% CPU time in a typical simulation.  Merry
Christmas.

  CRS, 12/21/98


  The current advection terms ~ div(JV+VJ) have been added to
brhs_hall.  They are used in Ohm's law when the input parameter ohms
is set to '2fl' and advect is set to 'all'.  The current density used
in these terms is a nodal quantity (computed only when these terms are
used), and it is predicted and corrected with the Hall time split.
Note that this may not be sufficient for numerical stability in a
dissipation-less system, since the prediction is time-centered,
whereas the finite element formulation require a somewhat forward
centering for stability.  [See the discussion regarding velocity
advection.]  In addition, cfl conditions related to the difference
between electron and ion flows may be required--this has not been
investigated.  [<--USER BEWARE-->] These advection terms have not been
thoroughly tested for accuracy, and bugs may exist at this point.
  A check of the electron flow cfl condition is now made in new_dt
when separate_pe is true, since the electron velocity is used for
advection electron pressure.

  CRS, 12/23/98


  The reversal parameter computation in toroidal geometry has been
corrected for the change to B_phi as a nodal quantity (instead of
R*B_phi).  Also, conditional statements nim_output have been modified
to avoid division by zero.
  Some minor corrects have been made to the seam set-up to handle more
general cases with a degenerate point at r=0.

  CRS, 1/6/99


  2_2_6

  Advection and viscous dissipation in the vrhs subroutine
(integrands.f) have been rearranged for optimization.

  CRS, 1/21/99

  The diffusivity shape has been converted to a bilinear
representation in the rblocks with the bilinear quantity diff_sh2 in
preparation for simulating external modes with a vacuum region.  A
velocity sink term has also been added; its magnitude in the vacuum
region is set by the new input parameter vsink_rate.  So far, this is
only linear.
  A vertical, poloidal electric field may be applied, similar to the
loop voltage.  It is set by e_vertical with ramp start and end times,
t_e_vert0 and t_e_vert1.

  CRS, 1/26/99

  The default value of extrap_order has been changed from 0 to 1 in
the nimset input.f file, and the default value of si_in_v has been
changed to true.  If si_in_v is true and kin_visc>0, we recommend
setting split_visc=true.

  CRS, 3/1/99

  The loop_voltage and vertical_efield routines in utilities.f have
been changed to allow ramps that decrease in time.  The 0-time
(tloopv0 or t_e_vert0) is still the time when it hits zero, and the
1-time is when it's at its full value of loop_volt or e_vertical.
  This state is being tagged nimrod2_2_6.

  CRS, 3/15/99

  2_3

  All changes from 2_2_6 to 2_3 are in non-kernel parts of the nimrod
package.

  CRS, 4/5/99


  2_3_1

  Changes here are mainly improving the rhs integrand routines for the
pressure advances.  Scott Kruger modified prhs to eliminate the
integration by parts (only needed for the defunct non-conforming option)
and to include the div(V_eq) term.  The perhs routine has been similarly
updated, and a correction to the ve cfl computation in new_dt has also
been made.
  There is also a bug fix to zero out the sink arrays in inviscid
computations in integrand routines.

  CRS and SK, 5/5/99

  A bug in the set-up for line Jacobi has been fixed.  This only
affected line solves in periodic blocks, but it makes a pretty big
difference in many cases.

  CRS, 5/27/99

  A global line Jacobi preconditioning option has been added.  The
option is 'gl_diaga', and the iter_gl_ld_* routines are used to factor
and solve the lines, which extend across as many adjacent rblocks as
possible.  They use the new parallel_line* routines in parallel.f for
both serial and parallel communication of line data.  This
communication represents decomposition swaps similar to the processor
layers used for computing Fourier components and FFTs/pseudo-spectral
operations in parallel at different stages.  The residual and
preconditioned residual are not averaged along block borders, since
the operation is redundant on different processors.  In addition, the
communication and restoration of matrix elements along block borders
has been modified, since seaming is done within this preconditioner
factorization, and seam_in gets over-written.  The new seam_save array
has been added to the seams for this purpose.

  CRS, 6/25/99


  2_3_2

  Anistropic diffusion has been introduced into the scalar pressure
equation, but not the electron pressure equation.  The pressure
closure model is controlled through the p_model flag in the
closure_input namelist. The options are either "adiabat", "iso",
"aniso0", or "aniso1."  The 'adiabat' option is for standard MHD
(p_si_op). The 'iso' option adds isotropic diffusion through k_perp to
the adiabat model (p_si_op).  The 'aniso0' option specifies
anisotropic heat diffusivity of the form q = k_perp grad P + k_pll b b
dot grad P where k_perp and k_pll are spatially dependent constants
(p_aniso_op).  This option bypasses all the flow dependencies in
normal MHD.  The 'ansiso1' option includes the flow dependencies
(p_aniso_op).  Two semi-implicit factors are associated with the
diffusion operators, si_fac_pll and si_fac_perp.  The parameter
si_fac_pll should normally be one and leads the anisotropic operator,
this is exact and a fully implicit solve when only n=0 is in a
calculation.  The parameter si_fac_perp leads the isotropic diffusion
operator (si_fac_perp*(k_perp+k_pll)) and is used to stabilize the
nonlinear terms of the anisotropic operator.
  A simplified neoclassical closure is also being added.  It
suppresses bootstrap current within the island width for a specified
(m,n).  Set neo_flag="Hole" to use this closure.

  TAG, 6/26/99


  2_3_3

  There was a bug in the velocity advection term that is now
corrected.  The centrifugal equilibrium flow terms were incorrectly
applied in linear geometry cases as well as toroidal geometry cases.

  CRS, 7/20/99

  The seaming order for block border vertices has been changed.
Simple wave tests were pathological for the previous scheme of testing
the magnitude of data and ordering in that fashion.  Now the order is
predetermined by global block and vertex numbers before the time step
looping starts.

  CRS, 7/23/99

  The second mass_mat structure (each incorporating different
regularity conditions for scalars) has been initialized incorrectly.
In matrix_init, it was allocated, set equal to the first, then
modified.  The equivalence acted like a pointer assignment instead of
copying values; so both structures were the same.  Now each element of
the second matrix is initialized separately, then the r=0 border is
modified.

  CRS, 7/30/99

  Initialization for the seam ordering was not carried-over to
nimplot, causing an error there.  This sent up a red flag for better
modularity.  So, I've reorganized what was in the edge_init routine in
nimrod_init.  The ordering initialization has been combined with
vertex array allocation and the initialization of ptr2 (all items
needed for normal vertex seaming), and this his been moved to the new
routine edge_init in module edge and file edge.f, so that it is no
longer duplicated in nimplot.  The segment initialization has been
moved to the new routine edge_segment_init in edge.  Creating the
weights used during cg solves has been moved to the new routine,
border_weights in iter_cg.f.  What's left of the old edge_init in
nimrod_init is the initialization of data for boundary and regularity
conditions, and that has been renamed boundary_init.  In addition,
block_create_tang, previously used by edge_init, and now used by
boundary_init, has been moved from edge.f to nimrod_init.f.

  CRS, 8/13/99

  A parameter switch, split_adv (coded default=false), has been added
to the global module as an interim aid for dealing with flow.
Previous versions have the advection predictor/corrector steps
separated with viscosity when split_visc is true.  This can enhance a
numerical instability associated with flow.  The default is now to
have the advection p/c together with mhd forces and the si operator,
regardless of split_visc.  Here are the present options:

  split_adv=F, split_visc=F:  same as old split_visc=F
  split_adv=F, split_visc=T:  new, adv not split from mhd, but visc is
  split_adv=T, split_visc=T:  same as old split_visc=T
  split_adv=T, split_visc=F:  split_visc is ignored and visc is split anyway

  CRS, 9/3/99


  quadratic branch:

  The following are changes for the quadratic branch of nimrod.

1.  The conform and use_delta options in generic, tri_linear and
quadrilateral evaluations have been removed.

2.  The edge module has been expanded to deal with complex data
directly and to handle side-centered data, which is communicated
through the edge_seg_network routine.  To do the latter, the segment
networking has been expanded to distinguish off-diagonal matrix
elements from local data.  Also, symmetry is not assumed in matrix
elements; this is for future plans for nimrod.
  Since the vector types have been expanded for side-centered data,
the tx_rhs option of edge_network calls edge_seg_network after loading
that data into the segment arrays.  In places where tx_rhs is not
used, there will be extra work to get the side data communicated.
  So far, corresponding changes to parallel communication have not
been implemented.

3.  The fft_nim routine has been modified for the complex form of data
with separate vector and Fourier component indices.  At present, the
real data is returned with vector component position third and
toroidal position fourth.  Parts of this update were lifted from the
crhs branch.

4.  The generic_evals module has been modified to use the new data
evaluation routines (lagr_quad and separate real and complex
tri_linears).  Since basis functions and gradients are now saved at
quadrature points, generic_alpha_eval just does pointer assignments to
the desired data.

5.  The math_tran module required modification for cross products of
different data types (real<->complex), which was lifted from crhs, and
the math_curl routine now uses the complex data form.

6.  The structures for storing rblock matrices have been expanded in
matrix_type_mod for the different basis types.  In particular,
rbl_mat_type (and the complex version) now has a 2D array (mat) of 6D
arrays (arr).  The lowest level is comparable to the bilinear-only
matrix array.  The 2D mat array now indicates basis function to basis
function interaction.  For example, mat(1,1)%arr is the
diagonal-in-basis interaction among grid vertex-centered coefficients.
However, mat(1,2)%mat is the influence of the first
horizontal-side-centered basis (quadratic in x, linear in y) data with
the grid vertex-centered test functions.  The mat array is dimensioned
(poly_degree**2,poly_degree**2), since there are poly_degree**2 basis
functions in a bi-whatever element.
  Allocation and deallocation routines for the rbl matrices have been
added to matrix_type_mod for convenience.

7.  The lumping routines in matrix_mod now handle the expanded matrix
format.  However, lumping may be taboo with nonstandard elements.

8.  The rblock module has been expanded for the possibility of
degree>1 basis functions in both the rhs and matrix routines.
Routines for setting and deallocating basis functions at the
quadrature points have been added.  The product of the Jacobian and
quadrature weight is stored and used for efficiency.  A second rhs
integration routine has been added for complex rhs data.

9.  The tblock module now has a rhs routine for complex data, but
higher-order basis functions have not been added yet.  Basis
set&deallocate routines comparable to the rblock versions have been
implemented.

  CRS, 9/23/99

10.  The solution and temporary vectors in iter_cg_comp.f are now
defined as the cvector_2D_type, which is similar to the cvector_type,
but does not have a Fourier index.

11.  The send_ and receive_rhs routines in edge have been replaced
with more general edge_load_ and edge_unload_ routines for vector_,
cvector_, and cvector_2D_ types, with passed vector arguments.  This
is much more general, since it may be used for vectors other than rhs
with loading and unload done outside edge_network.  It also simplifies
the job of communicating data located along the sides of elements.

12.  The iterative solver routines have been modified to handle data
at element sides and interiors and the change in array order.  At
present, the preconditioning only applies to grid vertex-centered
data, as before.  Diagonal preconditioning is used on coefficients
centered elsewhere.

13.  The matrix_mod module has been expanded for the higher-order
basis functions.  For each rblock, a matrix structure now has a 'mat'
substructure, which is a 2D array (pointer).  The indices roam over
the basis types.  For rblocks there are poly_degree**2 different bases
and the 6D 'arr' array under the 'mat' substructure holds the elements
connecting each basis type with every other basis type.  The 'arr'
array is therefore a generalized version of the old lmat array, with
the same type of sparse storage.  However, ordering has been changed
for the new vector array orders.  The new 6D array indices are defined
as (from-q,ix-offset,iy-offset,to-q,ix-to,iy-to), so that summed data
is as contiguous as possible in memory.  To be consistent, the mat
ordering at the next higher level is (from-basis,to-basis).  For
example, mat(1,1)%arr is the array containing grid-vertex to
grid-vertex connections, which are always used.  For quadratic bases,
mat(2,1)%arr holds connections from the bases centered on the
horizontal sides of elements to the grid-vertex bases, for example.
Symmetry is not assumed in this storage scheme; this is future
planning at the expense of memory.
  Matrix allocation and deallocation routines have been added to
matrix_mod.

14.  The block to line communication in parallel.f has been modified
for the new array ordering, with quantity index first.  Parallel
communication has not yet been updated.

15.  The matrix integration routines in rblock have been updated for
array order and generalized basis functions.  The integrand array
orders have been modified for consistency and efficiency, and this has
been done in tblock, too.
  Note that the integrand array now has a smaller dimension for rhs
computations than it does for matrix computations.  Furthermore, the
order of basis functions around a cell for the cell-to-basis
scattering is now grid vertices first (low-left, low-right, up-left,
up-right), horizontal sides (bottom to top, each hor side-cent basis),
vertical sides (right to left, each vert side-cent basis), then
interior bases (left-right, bottom-top).

  CRS, 10/5/99

16.  The boundary condition routines have been modified to handle
basis types centered at element sides as well as grid vertices.  The
treatment presently acts as if the bases are standard Lagrange types,
and it appears that a switch to these bases are necessary.  There may
be a bug in the matrix bc setting routines at this point.

17.  New complex 2D vector structures are used in some of the matrix
solves for both rhs and solution vectors.  Since the number of vector
quantities depends on the management routine, these structures are
allocated and deallocated for each solve.

18.  Diagnostic_ints.f has been updated to the new array order and
generic_eval call syntax.

19.  The dump file routines have been updated for the new data
structures.

20.  Side-centered normal and tangent vectors have been added to the
seam%segment structures for setting boundary conditions.  This may
need upgrading for better accuracy later.

21.  The finite_element module has been updated for the new rblock and
tblock integration calls and for the new data structures.  The calls
for regularity conditions are presently commented-out.

22.  The generic_evals module has a new generic_2D_all_eval for
evaluating real 2D fields such as be_n0 and neoclassical quantities.
The old generic_all_eval has been renamed generic_3D_all_eval, and an
interface is used so the call still appears as generic_all_eval.

23.  The history module has been partly updated.  Computations for
current, F and Theta do not use higher-order information at this
point.

24.  The integrand routines have been updated for the new array order
and evaluation call syntax.  Note that the passed int array now has
different dimensions for different types of integrands.  For real and
complex matrices, it's still a 6D array, but the ordering is now
(jq,iq,ix,iy,jb,ib), where jq is the column vector component, iq is
the row vector component, ix and iy are the cell indices, jb is the
column basis type, and ib is the row basis type.  For complex rhs
computations, int is a complex 5D array ordered (iq,ix,iy,ib,imode),
where imode is the Fourier component index.  For real rhs
computations, int is a real 4D array ordered (iq,ix,iy,ib).

25.  The complex conjugate gradient solver has been modified to use
only 2D vector types, so there is no hassle with Fourier component
indices.

26.  The neoclassical.f file has been updated for the new array order,
but it does not yet perform some of the computations for the
higher-order basis functions.

27.  The management routines in nimrod.f have been modified to use the
new routines in the vector_type_mod module for manipulating data for
all basis functions.  These operations include assigning one type of
vector (or part of it) to another (or part of it), scalar
multiplication of a vector, and summation of two vectors.

28.  The routines in nimrod_init.f required updating for the new data
structures.  The edge initialization routines have been broken-up and
moved into the edge module.  The remaining portion is the
boundary_init routine in nimrod_init.f, which must be called after
edge_init but before edge_segment_init.
  The lumped-mass arrays have been stripped (but may appear again
later), and the mass matrix is now stored as a full matrix structure.
The matrix_init routine now uses structure allocates found in
matrix_type_mod.

29.  The parallel_io.f file has been modified to transfer data for all
of the different basis types.  This has not been tested yet.

30.  So far regularity is just a dummy.

31.  The utility routines have been modified for the new data
structures and array orders, but higher-order information is not yet
used for the cfl and average field checks.

  CRS, 10/26/99

Standard Lagrange elements are used from this point.  The kernel
now functions, but preconditioner options other than diagonal are not
fully implemented for higher-order cases, and regularity conditions
are not available.

32.  Bugs in the matrix boundary conditions and in diagnostic_ints
have been fixed.

33.  With standard elements, side data along a degenerate side is
nonzero.  Appropriate seaming with vertices along that side has been
implemented in edge_network.  The collection of matrix elements for
the degenerate side has been moved to matrix_mod from finite_element,
so it can be accessed more widely, and it has been expanded to address
connection among the side-centered data.

34.  Parallel seaming has been updated for: 1) complex vertex
communication, 2) real and complex segment communication, 3) separate
segment communication for real and complex off-diagonal matrix
elements (without assuming symmetry).

35.  Isotropic semi-implicit operators no longer use a conditional to
go to zero under explicit conditions.  This caused problems with the
higher-order approximations, and isn't necessary.

36.  The averaging factor used for segment-centered data in the dot
products in the iterative solvers has been set to zero for side data
along degenerate boundaries.

37.  Diagonal matrix elements for side centered data along block
borders is communicated for the preconditioners.

38.  The matrix lumping routines in matrix_mod are now functional for
all polynomial orders.

39.  Logical variables are now used for tracking n=0 and n/=0 changes
in pressures.

40.  The lump_all option is again enabled.  Lump_b is not so far.

41.  Bugs in the arrays sizes of side and interior centered data that
are used in the mpi calls have been fixed in parallel_io.f.

42.  The input parameter ngr has been changed, so that the number of
quadrature points increases with poly_degree with fixed ngr.  The # of
point in each direction for rectangular cells is now
ngr+poly_degree-1.

43.  Matrix couplings to side data along degenerate borders of rblocks
are no longer zeroed out.

44.  The pcnst option for met_spl has been restored.

45.  The basis functions in degenerate cells are modified when lumping
is used to prevent zeros on the lumped mass diagonal for quadratic
elements.  When quadratics are used without lumping, the mass matrix
is not diagonally dominant for these rows.

46.  The ave_field_check routine has been modified to update side and
interior centered n=0 data when needed.  The nonlinear pressures are
now checked and updated separately from the n=0.  The data that stores
the nonlinear pressures is presently bilinear regardless of the
polynomial degree used for other quantities.

  CRS, 12/9/99

47.  The tangent and normal unit vectors along element segments for
polynomial degree > 1 are now computed with bicube evals at the
location of the data.  The seam%segment arrays have a second index
over the number of side elements.  These computations have been moved
from edge to block_create_tangent in nimrod_init.f, which is now
called separately from edge_init.
  Routines in the boundary module use the specific unit vectors for
each rhs vector and matrix element.

48.  The extrapolation routines and calls have been updated to use
complex data with vector component first and to allow higher-order
data.

49.  While testing, the following bugs were found and corrected:
  a) grad(pe)_phi in brhs_hall was missing a factor of i.
  b) array dimensions in brhs_etaj were not set correctly.
  c) the tblock matrix unloading in iter_mat_comm and iter_mat_rest
     did not have the revised order.
  d) the divb_fac structure now need to have dimension 3 for all
     Fourier components including n=0.

  CRS, 12/13/99

50.  The vector component index of sln and rhs is now assumed to have
dimension nqty in iter_cg_comp.f and iter_cg_f90.f for optimization.
This is less general than the previous implementation.

51.  The matvec routines for rblocks have been broken into separate
subroutines for each possible combination of operand basis type and
product basis type.  This has been done for optimization.

  CRS, 3/7/00

52.  The block and global line Jacobi preconditioning schemes have a
new elimination step.  Along each line, data from the cell interiors
and from the segments parallel to the line being solved is eliminated
to reduce the 1D matrices to block-tridiagonal, regardless of basis
function degree.

53.  The tangent vectors at rblock corners for setting external
boundary conditions now receive special treatment in
block_create_tangent.

54.  5-node (per direction) quadrature has been added to rblock_set.

55.  Definitions for Lagrange bicubics have been added to
lagr_quad_mod in lagrange_quad.f.

  CRS, 4/10/00

56.  Routines for eliminating cell-centered data in quadrilaterals
(biquadratic or higher basis functions) have been added to matrix_mod.
These routines are called automatically from matrix_create (in
finite_element_mod) and get_rhs.  The iterative solvers have also been
updated, so that their treatment of cell-interior data is optional in
accordance the new matrix structure flag 'eliminated.'  After a matrix
solve is complete, the interior data is found through a call to
fe_postsolve, which finds appropriate routines in finite_element_mod.
The extrapolation routines have also been changed, so that interior
data is not extrapolated for solves not using it.

Management routines have received minor changes for this elimination:

1) matrix_create must be called before get_rhs, so that the
elimination of rhs data may take place before the seaming call.

2) Calls to get_rhs pass the matrix structure as an optional argument
if elimination is desired.

3) fe_postsolve is called after the iterative solve.

During set-up, matrix_init in nimrod_init.f calls iter_fac_alloc with
a flag indicating whether elimination will be performed.  This
influences how much space is allocated for the line Jacobi
computations.

Note that "bare" mass matrix solves do not use elimination, since the
mass matrix is also used to construct other matrices, and since these
solves take relatively little time.  [P advance without thermal
conduction or an si op, the solve for J, and the solve for
neoclassical parameters are examples.]

57.  To make the elimination a little easier, the dt factor for the
rhs of the velocity advances has been moved into the integrand routine
vrhs.  It no longer appears in the management routines.

  CRS, 4/17/00

58.  The routines for enforcing regularity conditions at R=0 have been
added back into this branch.  They have been updated to handle the
general basis functions and the change in array order.

59.  The data save for extrapolating solution guesses has been moved
back to where it was previously, right after the iterative solution.

  CRS, 4/20/00

60.  The quadrature weights and abscissas in rblock.f are now
generated by an algorithm from Numerical Recipies.  This has been
added to complete the modifications for unrestricted quadrilateral
polynomial degree.

  CRS, 5/24/00

61.  Array index order for element side and interior centered data has
been changed, so that the basis index is second, immediately following
the vector component index.  This allows us to treat all bases for
each basis type (grid vertex, horizontal side, vertical side, and
interior) as expanded vector component indices during matrix-vector
multiplication.  The second from lowest structure level of rblock
matrices is therefore a 2D array over basis type, instead of over
bases.  At most it is a 4x4 array.  The quantity indices at the lowest
level are expanded for all but the grid vertex to grid vertex matrix
arrays.

  CRS, 6/9/00

62.  A bug in the computation of nsh and nsv in routine
gather_lagr_quad has been fixed in parallel_io.f.

63.  Integrand matrix routines, rblock matrix gathers, and
multiplications performed during the line Jacobi preconditioning have
been optimized.  [The efficiency of finite element computations
becomes more important as the polynomial degree of the basis functions
increases.]

  CRS, 6/15/00


  3_0

  The quadratic branch is being merged into the main trunk.

  CRS, 6/28/00 

  A bug has been fixed in the tblock_basis_set routine.  For toroidal
geometry the wjac (weight times Jacobian, stored at the nodes) needed
to be multiplied by R.
  The mpi module name has been changed to mpi_nim, and "io" is no
longer used as a local variable in the iterative solvers, to avoid
getting the Portland Group compiler confused.

  CRS, 7/14/00

  Poloidal Flow damping forms have been added to the momentum
equation: HandS, pfd, and pfd2.  HandS is the Hirshman and Sigmar
formulation, pfd is a poloidal flow damping form derived from vector
directions based on HandS, and pfd2 is a poloidal flow damping form
based exclusively on the equilibrium poloidal magnetic field.
  Implementation of new closures in the Ohm's law for the perturbed
boostrap current: HandS, pp1, pp2, pfd, pfd1, pfd2, and Hole.  In
level of approximation, the closures are HandS is the Hirshman and
Sigmar formulation, pp1 and pp2 are perturbed pressure variants that
require anisotropic heat diffusion based on vector directions from
HandS, and Hole uses an analytic pressure profile about the island
surface for a bootstrap current in the parallel direction.  The pfd1
and pfd2 are based on a poloidal flow damping form for the vector
directions and the current that should be damped/driven.
  The neorhs and pfdrhs have been moved from integrands.f to
neoclassical.f and made part of a the neoclass_int module.  This
isolates more of the neoclassical effects from the main code, with the
exception of brhs_mhd and vrhs.  The allocation routines have also
been removed from nimrod_init.f and placed as a separate routine in
neoclassical.f.  This helps avoid problems with changing array sizes
in neo_mp.
  The transition to generalized elements required extensive rewriting
of the flux averaging routines used in the setup of the neoclassical
closures in neo_init.  The flux-averaging is now accomplished with the
lsode routine and so linking of lsode is now required also for
nimrod.f

  TAG, 9/22/00

  The get_rhs routine had an error when eliminating interior data for
real matrices if nqty/=3.  The flags deciding which data to transfer
to the temporary real arrays were always set to r12mi3 and i12r3,
which is correct for nqty=3 only.  This caused segmentation faults in
adv_p_iso with isotropic diffusion, since nqty=1 and the matrix is
real.  This did not affect anisotropic diffusion, since that uses
complex matrices.

  CRS, 10/01/00

  New subprograms have been added to allow finite element computations
of surface integrals that appear on the rhs of equations (usually
associated with some kind of flux).  The new surface module contains
the numerical integration routine, and the surface_ints module
contains the integrand routines, where different physical effects may
be added (like rblock and integrands, respectively).  To help
facilitate these changes, the 1D polynomials used to define
lagrange-type basis functions, and the Num. Recipies routine for
generating quadrature points have been moved to the file
polynomials.f.
  To accommodate surface integrals, there are two new parameters in
the calls to get_rhs.  The do_surf parameter is a logical that selects
the surface integration block of coding, and surf_int is the name of a
routine in surface_ints that contains the physics appropriate for a
particular equation.
  The only equation using the surface integral is the magnetic
advance, where applied electric fields lead to a flux of magnetic
field.  This replaces the localized but volumetric e_applied source
density, so e_applied_init in nimrod_init.f is just a shell.  The
source density coding may be useful for other things later.
  The surface_exb routine in nimrod.f has been updated for
higher-order elements.

  Another significant change is that the velocity equation is now of
the form

 rho*DV/Dt=force

instead of

 DV/Dt=force/rho

so that the linear MHD force operator is self-adjoint when there is a
density gradient.  The mass matrix operator that was added separately
now appears in the routines that create the semi-implicit operators,
since it now has the rho factored in.  There is a new optional switch
in the matrix_create parameter list, mass_type, which is set to 'none'
to avoid the default of adding a mass matrix these operators.

  CRS, 12/04/00

  The integrands, matrix_mod, iter_cg_real, and iter_cg_comp modules
have been broken into separate modules to facilitate compilation with
IBM's xlf compiler.  The optimizer chokes on modules that are large.
  There are also a couple of bug fixes.  In the iterative solvers, nrb
was used before being defined in iter_factor, and the denerate point
check for matrices in finite_element (bl_drect preconditioning only)
was not upgraded to vector component first ordering.  Also,
ALLOCATABLE arrays in vrhs in integrands were not deallocated.

  CRS, 12/14/00

  Calls to nonblocking communication in parallel.f have been modified
to pass the first element of arrays located in structures instead of
the entire array.  This seems to avoid some mpi problems on Linux
machines and when using the AIX operating system.

  CRS, 12/19/00


  3_0_1

  A conjugate gradient solver for 3D matrices has been added.  It is
contained in the iter_3d_cg_mod module in file iter_3d_cg.f.  The
algorithm uses a matrix-free approach, so that the full 3D matrix is
not needed.  Instead, an integrand routine for a rhs-type finite
element computation for the matrix-vector product is passed into the
solver, and this is called every cg iteration through get_rhs.  The
get_rhs routine in finite_element module also had to be modified, so
that the rhs data structure is passed through the calling statement.
The passed structure is assumed to have the correct dimensions for a
given computation, so management routines now allocate and deallocate
storage for each computation.
  The 3D solver uses the preconditioners in iter_cg_comp for 2D
matrices (each Fourier component independent).  [The preconditioning
routines in iter_cg_f90 and iter_cg_comp have been slightly modified
to allow calls from a routine external to the 2D matrix cg modules,
where they reside.]  Thus, one still needs to form a set of 2D
matrices representing the diagonal-in-n part of the 3D matrix, as
these matrices are needed to find the approximate factors used in
preconditioning.
  Since the full 3D matrices are not found, it is not possible to
eliminate element-interior data prior to the start of a 3D iterative
solve.  Eliminations with the diagonal-in-n part of the matrix are
performed each iteration as part of the preconditioning, however.

  The matrix-free 3D iterative solve puts a much larger burden of
efficiency on the rhs finite element computations.  To help
optimization, storage for the values and derivatives of solution
fields at the Gaussian quadrature points has been added to
rblock_type_mod and tblock_type_mod.  Polynomial interpolations for
each solution field (and its predicted value) are performed once per
time step instead of every time it is used in an integrand routine.
The integrand routines now have pointer arrays that are set to access
this storage, instead of computing the interpolations.
  Changes for the quadrature point data include: 

1.  qp allocate, deallocate, and update routines in the rblock and
tblock modules.

2.  Integrand routines now call generic_ptr_set instead of
generic_all_eval in most places to access the quadrature point
storage.  For temporary work arrays that are only used once,
generic_all_eval is called to perform the interpolation on the fly.

****CAUTION****
When programming integrand routines, one must be aware that the target
arrays for the pointers must not be changed.  Doing so will affect the
quadrature point data used later in the time step.  This changes the
way many arrays are used in integrand routines.
***************

3.  The equilibrium B_phi and J_phi and their derivates are stored as
cylindrical components (instead of covariant and contravariant) at the
quadrature points to avoid repetitive operations with R.

4.  New routines, generic_2D_ptr_set and generic_3D_ptr_set, have been
added to the generic_evals module.  They are accessed by the common
generic_ptr_set interface.  In addition, generic_eq_all_eval now does
the interpolation for a bicubic spline, since its former function
is encompassed by generic_2D_ptr_set.

5.  The ave_field_check routine in utilities.f updates symmetric
fields and nonlinear pressures at quadrature point positions, as
needed in nonlinear simulations.

6.  Management routines in nimrod.f update quadrature point
storage after the matrix equation is solved.

7.  The routine quadrature_save in nimrod_init.f replaces bicube_save.
It initializes the solution fields at the quadrature point in addition
to setting equilibrium fields there.  Data structures for the solution
fields are allocated in variable_alloc.

8.  Information on quadrature points (such as ng) is now part of the
block types, since routines outside rblock and tblock need to work with
quadrature point information.

  The first use of a 3D matrix is for anisotropic thermal conduction.
The adv_p_aniso management routine calls the 3D iterative solver
passing "p_aniso_dot" as the name of the integrand routine for finding
the 3D-matrix-vector product.  p_aniso_op now forms just the
diagonal-in-n part of the 3D matrix for creating a preconditioner.
The n=0 part of the (b_hat)(b_hat) dyad is computed with the find_bb
utility routine.  It approximates the B**2 in the denominator as
(B_eq+B_n=0)**2 to produce a closed form for the n=0 part of the dyad.
This introduces some error in the magnitude of the parallel
diffusivity, but not in the direction.  The same approximation is used
in prhs and p_aniso_dot for consistency (important).
  Centering for thermal conduction is set by the numerical input fthc;
the old si_fac_pll and si_fac_perp have been removed.

  Other changes in 3_0_1:

  The surface_exb routine has been moved from nimrod.f to utilities.f,
and the velocity data structure is passed along with time-centering.
The flow boundary condition is now applied to velocity predictions at
the centering specified by fv_vdgv, which was not done in the past.
  The "_old" data structures have not been used for some time and have
been removed (along with the shift routine) to save space for the new
quadrature point storage.
  The extrap_eval_cvec routine has been added to extrap_mod.  It is
used to simultaneously extrapolate all Fourier components for the next
iterative solver guess.
  The dump module has been broken into separate read and write modules.
  There is a bug fix in history, so that computation of I and phi
include blocks of triangles.
  The component checks in boundary.f have been modified to prevent a
segmentation error.
  Integrand routines have been optimized to avoid repeating basis
loops.

  CRS, 4/10/01


  3_0_2

  Minor changes are being made to clean-up before adding continuity.

1.  A recent bug in integrands, which added the equilibrium flow
inertial force to garbage in linear runs, is corrected.

2.  A fairly recent bug in the Hall centering in adv_b_iso is fixed.

3.  Scott K. and I found that the new iter_3d_cg module assumed nqty=1
in some places.

4.  si_in_v is no longer an option.  Semi-implicit operators in the
mhd advances of b and p are removed.  The adv_b_aniso and
curl_de_aniso routines have therefore been removed from nimrod.f and
integrands.f, respectively.

  CRS, SEK, 6/4/01

  In the semi-implicit operator, the b_n=0_phi/R term for toroidal
geometry was added to J0_z even in linear geometry.  This has been
corrected.

  CRS, 6/8/01

  In neoclassical.f and integrands.f, additional changes to the
electron and ion neoclassical viscous stress tensor closures.

  TAG, 6/14/01

  Bux fix: the energy_density subroutine in diagnostic_ints set a
pointer to qpres, regardless of whether it had been allocated or not
(depending on beta).  Conditional statements have been added to fix
this.

  CRS, 8/22/01


  3_0_3

  Changes for using 3D evolving particle number density have been
made.  So far this includes the continuity equation itself and the
number/mass density that appears in the center-of-mass velocity
equation and the 2-fluid magnetic advance.  It does not include the
separate electron pressure advance.  Furthermore, preserving the
self-adjoint properties of the advance will require temperature
equations instead of pressure equations.  These change will be made
shortly.
  There are 4 options for density evolution, specified with the new
input parameter, continuity.  These are 1) skipping n evolution (as in
the past), 2) evolving it but not using it, 3) using just the n=0
part, and 4) using the full 3D evolving n.  See the input.f file in
the nimset directory for more information.

Coding changes include:

1.  ndrhs (essentially Alfonso's coding) is added to the integrands
module, and the management routine adv_nd is added to nimrod.f.  The
density is advanced immediately following the advance of V.

2.  nimrod_init has a parameter-dependent initialization for
quadrature point storage for the 3D number density and for the n=0
only part (nd_n0) that is copied to all Fourier layers when running in
parallel.

3.  fields, rblock- and tblock_type_mod's have additional storage for
density data structures.

4.  The ave_field_check routine in utilities.f now checks for density
changes, so that matrices will be recomputed when total n changes
greater the the ave_change_limit tolerance.

5.  Since mass density appears on the lhs of the center-of-mass
velocity equation, there is a new management routine, adv_v_3dn, in
nimrod.f that calls the 3d iterative solver when full continuity
evolution is used.  It uses the matrix-vector product routine
v_aniso_dot, which is an rhs-type fe integrand routine in the
integrands module, to perform the matrix-free cg iterations.  The
actual rhs integrand, vrhs has been modified extensively to account
for the possible 3D mass density that appears in the inertial and
viscous force terms.  2D matrix routines for V have also been modified
to use the evolving n=0 part of density as needed (including creating
a matrix for preconditioning the 3D solve).

6.  The split_adv programmed option (formerly set in global.f) has
been removed to avoid further complication in the selections made
during the time-step advance.

  CRS and AT, 6/15/01

  A quick list of fixes:
1.  in vrhs, ve_eq ptr was not set for viscosity unless advect/=none.
2.  eq_vadv computation used uninitialized arrays and had tor geom only.
3.  in nimrod_init, visc_mat is now created for all split_visc cases.
4.  in nimrod.f, dum_fac in adv_v_3dn has to be nullified for mat_cr.
  
  CRS, 8/23/01


  3_1

  The changes made for this version complete the use of number density
evolution with respect to internal energy.  Thermal conduction is now
effected by temperature gradients, instead of pressure gradients, and
sound waves will propagate in isothermal conditions.
  In order to be true to the self-adjoint form of thermal condition,
we now evolve temperatures as  fundamental variables, instead of
pressures.  The full evolution equation for each species temperature
is

    n*dT/dt=-(gamma-1)*n*T*div(v)+div(n*chi.grad(T))+(gamma-1)*Q

where Q includes heating terms and thermal equilibration with the
other species (if separate_pe=true).  Note: 

1) The number density used in this equation (3d vs. n=0 part, etc.)
depends on input specified for the continuity input parameter.

2) The diffusivity, chi, can be an anisotropic tensor with separate
parallel and perpendicular coefficients for each species.  This is the
p_model='aniso1' option (as before with pressure evolution).

3) When 'lesser' p_models are used, such as isotropic diffusion, we
use the approximate form,

    dT/dt=-(gamma-1)*T*div(v)+div(chi.grad(T))+(gamma-1)*Q/n

where chi is now a diffusivity on T, rather than a proper thermal
diffusivity.  Adiabatic and isothermal forms are also available--see
input.f.

4) When separate_pe is false, the perpendicular ion diffusivity and
parallel electron diffusivity are used in the ion equation, and we set
Te by

  Te=pe_frac*Ti/(Zeff*(1-pe_frac))

so that the electron pressure is pe_frac of the total.

5) Ohmic and viscous heating are available.  When separate_pe is true,
Ohmic goes to electron energy, and viscous goes to ion.

  The implementation uses the same management and operator integrand
routines for both species.  The rhs integrand routines are separate
(tirhs vs. terhs) because the computations for Vi and Ve are different
enough to be confusing if put together.
  Separate storage for pressure is still used to simplify various
computations and to avoid re-coding neoclassical terms.  Pressure is
computed locally at the finite element nodes (as the product of n and
T coefficients at the nodes) and then treated with the same basis
functions used for other dependent variables.

  CRS, 11/20/01


  A new utility routine, qp_fft_save, has been added for the purpose
of saving real space (f(phi)) at the quadrature points.  This avoids
repeating ffts of the same data in integrand routines, which was
particularly wasteful in matrix-vector computations for 3D linear
systems via matrix-free.  At present, only magnetic field and number
density are saved this way, and the equilibrium contributions are
included.

  CRS, 1/14/02

  Expanded use of the saved nd_total and be_total has been implemented
in tirhs, terhs, vrhs, etc.

  CRS, 2/19/02


(nimuw) 3_1

  Changes for temperature-dependent resistivity have been added
(possibly separate from the permanent repository).  The resistivity
used in the computation is controlled by the new input parameter
eta_model, which allows one to chose between fixed resistivity, 2D
resistivity dependent on the symmetric part of Te, or 3D resistivity
based on local Te.  The form of temperature-dependent electrical
diffusvity is

MAX(0, MIN( elecd_max, elecd*(eta_t_ref/(Te_eq+Te))**3/2 ) )

as evaluated at the quadrature points for the numerical integration
only.  The new qp data qelecd_phi holds this computation for 3D
resistivity (eta_model="eta full"), and elecd_n0 holds the toroidal
average of this for "eta full", but elecd_n0 holds the computation
based on the symmetric Te for "eta n=0 only".  In the full case, the
n0 data is used to construct the preconditioning matrices.
  The new adv_b_3deta is the management routine for 3D magnetic
advances.  The 3D diffusivity is computed with find_eta_t in
utilities.f, and the matrix recompute criterion is determined with
ave_eta_check also in utilities.f.  Besides the modifications to
brhs_mhd in integrands, there are two new integrands.  b_3deta_dot is
the matrix-free matrix-vector product computation used in the cg
iterations, and curl_de_ciso is used to find the preconditioning
matrix.  For preconditioning, curl_de_iso is not adequate, since a
single complex matrix is needed for each Fourier component.  There
have also been changes to tirhs and terhs to compute Ohmic heating
with the Te-dependent resistivity.
  One should note that the resistivity storage is total resistivity.
An "equilibrium" resistivity based on te_eq is saved in elecd_eq and
is used for electric field arising from (delta-resistivity)*J_eq.

  Start and end cell indices for the 1D poloidal indexing used during
pseudospectral computations has been added to global: ipolst_block()
and ipolen_block(), which work like mps_block() with ipseust and
ipseuen set by get_rhs like mpseudo.  The smallnum value has also been
added to global.  It is set to SQRT(TINY()) and is used to avoid
dividing by zero in various places.

  CRS, 3/28/02

  A restart capability has been added to the iterative solvers.
Restarting here means throwing out the direction vector and using the
iterate as a guess for a new series of cg steps.  It is invoked if the
error grows over a number of iterations or if the norm of the true
residual differs significantly from the norm of the recursive residual
that is computed with the cg steps.  The norm computations and true
residual computations have also been separated into subroutines.

  CRS, 5/10/02

(nimuw) 3_1_1

  The split_resist option has been removed to avoid dragging along
outdated coding that isn't used and isn't capable of
temperature-dependent resistivity.
  The Dirichlet-type regularity conditions are now complete for cg
iterations during 3D matrix solves.  For the homogeneous conditions,
the call to regular_vec in get_rhs takes care of it.  [Regular_vec
zeros-out appropriate components, and makes v_r=v_r-i*v_phi for n=1,
creating the summed-eqn rhs.]  For the n=1 condition, v_r=-i*v_phi,
the following sequence of calls is now observed for 3-vectors:

1. Call regular_ave, setting dir_phi=i*dir_r
2. Call get_rhs to find A.dir, which includes regular_vec after the
   matrix-vector multiplication
3. Call the new regular_zero_phi, (A.dir)_phi=0

before the possible 2D matrix-vector multiply.  This sequence is then
equivalent to the matrix-vector multiply where the unknown n=1 Fourier
components at r=0 are (x_r-i*x_phi)/2 and x_z, and the r and -i*phi
rows are summed, since the saved 2D matrix has gone through
regular_op.
  The d/dr conditions have been removed, since an examination of the
weak form of the equations show that these conditions will be
satisfied naturally through mesh refinement, despite the fact that
there is no surface.  [I had tested an explicit diffusion-penalty
operator, and it produced results that tended to look worse than
having no extra penalty.]
  An ancient bug in nimrod_init, which set j_mat structures equal to
one another in nimrod_init (inadvertently moving pointers around) has
been corrected.  I think this had been corrected in the past, but it
may have crept back in with branch merges or discards.

  CRS, 6/20/02

  The ohms="mhd&hall" and ohms="hall" options now include the
electron pressure gradient term, like ohms="2fl".  In addition, the
default value of ohms has been changed to "mhd".

  GAC, 6/20/02

  Linear computations with anisotropic thermal conduction were
blowing-up immediately if either heating term was used.  The code was
solving for temperature by evolving the linear dT/dt, not n*dT/dt, as
indicated in the comments, but the heating terms were multiplied by n.
To make it self-consistent and to treat equliibrium number density
gradients correctly, the linear calculations now evolve n*dT/dt, like
nonlinear ansiotropic evolution.  However, formally there should be an
n-tilde*(grad Veq):(grad Veq) term, which is still skipped in linear
cases.
  This led me to a bug in find_bb in utilities.f.  For nonlinear cases
with anisotropic conduction and continuity="fix profile" or "none",
the bb tensor was being multiplied by nd_n0 instead of nd_eq.  This
makes the 2D matrix for preconditioning a terrible approximation;
hence temperature iteration counts were much larger than when running
with evolving density coefficients.  There was also a careless "rb" in
the tblock section.  Finally, code for dividing by total B**2 instead
of <B>**2 has been removed.

  CRS, 6/21/02


  uw3_1_1map branch

  This branch is being used to complete the changes from cubic splines
to Lagrange polynomials for the RZ map and for equilibrium fields.
The dump files no longer contain any cubic spline information.
Changes to the physics kernel are pretty minor.  In most cases, the
changes are transparent at the level where the information gets used
(integrands automatically adjust thanks to generic_ptr_set, and only
vertex information is used for finding CFL limits).  However, since
diff_shape is now a lagr_quad_2D_type, diff_sh2 has been eliminated.
The evaluation of tangent and normal unit vectors along the domain
boundary has also been modified for the new map.

  CRS, 3/13/03

  The computation of tangent vectors in block_create_tang in
nimrod_init.f has been modified to average the limits of the tangent
vectors of the two elements adjacent to each grid vertex.  The
treatment of corners is particularly improved for the new rect_cir
gridshape.

  CRS, 4/04/03

  nimuw 3_1_2

  The uw3_1_1map has been merged into the main trunk of nimuw.  There
are also several less significant changes that are being made at this
time:

1) The vadv_eq computation of V0.grad(V0) in vrhs has been missing a
couple of factors of 1/R and is now corrected.  The error would have
affected toroidal geometry simulations with equilibrium flow and
continuity evolution.

2) The initial value of various fields are now loaded into the
extrapolation arrays during extrap_init to help the iterative solves
on the first couple of time steps.

3) A direct solve of small general nxn complex systems has been added
to math_tran.f in the form of subroutine math_solve_q1_cnxn for static
condensation with non-Hermitian matrices.

4) The cg solvers now have 2-norm computations available, though they
are not presently used.

5) The mpi_serial.f routine has more dummy routines for use with
AZTEC-compatible code.

6) The line Jacobi operations in the conjugate gradient solvers have
been modified to avoid function calls at the lowest level of loops to
help performance.

  CRS, 5/8/03


  post nimuw3_1_2

  The neoclassical closures have been shelved temporarily to avoid
extra compiling during planned development.

  CRS, 5/13/03

  nimuw3_2

  Commonly used modules have been moved to the nimlib libarary.  The
only file deleted from the nimrod directory is math_tran.f; however,
makefile have been changed to access nimlib.

  CRS, 5/15/03

  The advance routine and management routines in nimrod.f have been
modified to restart a time step with a reduced dt if a linear solve
doesn't converge.

  CRS, 5/19/03

  The bl_drect preconditioning option has been updated to handle
high-order elements and degenerate rblocks.  This implementation
considers only one block at a time and may not work for multiple
blocks.

  CRS, 6/17/03

  The "bl_drect" preconditioner option has been replaced by three: 

1) "lapack" -- calls LAPACK for a serial banded solve
2) "seq_slu" -- calls Sequential SuperLU for a serial sparse solve 
3) "slu_dist" -- calls SuperLU_DIST for a parallel sparse solve 

All perform global direct solves for the entire collection of
rblocks, regardless of poly_degree.

  The appriate libraries must be loaded when creating the executable.
See the new make.inc files in the make_includes directory for
information on how to do this.
  The makefile structure has been changed to place all device-specific
information in the make.inc files in the new make_includes directory.

  CRS, 6/19/03

  Minor corrections before applying the nimuw3_2 tag:

1. Extrap_init_array skips fields with 3D solves, since they save and
extrapolate the change over a time-step, instead of the field value.

2. The nd_n0 pointer in vrhs is now set only when continuity is 'n=0
only' or 'full.'  

3. The new input variable, zero_bnorm, is a logical that determines
whether to zero out the entire normal component of B (solution, not
equilibrium) or just its change.  The default is true--same behavior
as before.

4. A small but nonzero value is used to check if a boundary point is
an R=0 point when geom='tor.'  This seems to be needed on some Linux
machines, where a zero from nimset does not come up as a zero in
nimrod.

  CRS, 7/28/03

  nimuw3_2_1

  The new iter_dir_nonsym module provides a simple Jacobi wrapper for
calling SuperLU, and possibly other solver libraries in the future,
for nonsymmetric systems where cg is not applicable.  The
factorization is identical to that used in iter_cg_comp, so one should
call the iter_fac_alloc_comp with precon='slu_dist' or 'seq_slu' to
set-up storage and the mapping to compressed column format.  To factor
the matrix, call iter_factor_comp.  [Iter_fac_alloc_comp and
iter_factor_comp are part of iter_cg_comp.]  
  A related change is to break the iter_resid, iter_dot, and iter_norm
routines into a separate module, iter_utils, so that they can be used
outside of the cg algorithm.

  Besides the linear solver, there are other preparations for handling
nonsymmetric 2D matrices.  The matrix types now have a symmetric or
hermitian flag, which is set when a matrix data structure is
initialized.  This will affect the way static condensation is
performed, and it affects the collection of off-diagonal matrix
elements around a degenerate point.  The computations that have been
modified are in matrix_mod.

  CRS, 9/10/03

  nimuw3_2_2

  This update has minor corrections handling for nonsymmetric
matrices.  The fac_dir routine in iter_cg_comp needed to load the
columns of nimrod matrices instead of the conjugate of the rows, which
had been done for Hermitian matrices.  This change has also been made
for symmetric matrices in the fac_dir routine of iter_cg_f90; though,
we haven't tried any nonsymmetric real matrices at this point.  In
addition, the static condensation for real matrices
(matelim_real_inv_int in matrix_mod) and the degenerate point
collection routine can now handle nonsymmetric matrices.

  CRS, 9/26/03

  The timer in mpi_parallel.f is moved outside the mpi_nim module.
Parallel executables will now use this timer (calling mpi_wtime)
exclusively, whereas previously it was only called from subroutines
using the mpi_nim module.
  The *_old pointers for the solution fields (reintroduced at
nimuw3_2) have their pointer declarations moved to the fields module
to ensure that they are saved from time-step to time-step in a logical
place.
  Scott K's fix for the qelecd_eq initialization has finally been
added, preventing a possible divide by zero.

  CRS, 12/5/03


  nimuw3_2_3

  There are some minor bug fixes.  For ohms=="hall" (only), the eta*J
explicit electric field was missing a factor of mu0, probably since
adding the current advection term.  Also, the n=0 part of number
density has been added to the wave speed computations in newdt in
utilities.f, and ave_eta_check had misplaced parentheses for the eta
n=0 only option, forcing unnecessary matrix recomputes.  Finally, the
mpi_wtime timer in mpi_parallel.f has been commented out.

  CRS, 1/15/04

  The behavior of the temperature-dependent resistivity options has
been changed for linear computations.  Using "eta full" for linear
runs is now valid, and it turns-on the linear resistivity perturbation
for all Fourier components, based on the linear temperature change.
For "eta n=0 only" there is no resistivity perturbation for linear
runs, but both of these options use a fixed symmetric resistivity
profile that is computed from the equilibrium electron temperature
profile.

  CRS, 1/17/04


  nimuw3_2_4

  A change to new_dt in utilities.f adds the n=0 component of number
density to the equilibrium density for cfl computations when
continuity="n=0 only" or "full".
  There are now separate solver input-parameters for the vmhd, bmhd,
and temperature advances in case different choices help a computation
run faster.  If any of them are not specified, the respective advance
will use the choice specified by the original solver parameter.  At
present, one cannot mix global solvers (seq_slu, slu_dist, gl_diaga,
etc.)  with block-local solvers (bl_diaga, bl_diagx, etc.) due to the
different parallel initializations.
  There are new p_model choices for temperature-dependent parallel and
perpendicular thermal diffusivities with separate coefficients if
electron temperature is evolved separately.  [For separate_pe=F, use
k_perp and k_pll, and for separate_pe=T, use k_perpi, k_plli, k_perpe,
and k_plle.]  These models are only applied in nonlinear simulations,
and the parallel coefficient is based on T(R,Z,phi)**5/2, whereas, the
perpendicular coefficient is based on <T>**(-1/2) and <B>**(-2), where
<> is the average over phi.  See input.f for more details.
  For nonlinear simulations where total number density may approach
zero, a new local enhancement to nd_diff is applied to fill-in holes
in the equilibrium+n=0 component.  The local diffusivity goes to
dx**2/dt for average densities below nd_floor, with hyperbolic-tangent
smoothing of the diffusivity function.  See the comments for nd_floor
and nd_exp in input.f.  If nd_floor is not zero, the nonsymmetric
components of nd are also limited to have a magnitude of no more than
1/2 of the local nd_eq+nd_n0.

  CRS, 9/3/04

  The matrix_create routines in rblock.f have two different
scatter-like operations for combining data from the numerical
quadrature points into matrix elements.  Both are tested and timed for
each matrix in a computation, and the faster of the two is used on
subsequent calls.  The original version of the scatter operation does
poorly on large grid-blocks, presumably due to poor cache usage, but
is has fewer storage operations and does better on smaller blocks.

  CRS, 12/20/04


  nimuw324visc branch

  The viscous stress tensor appearing in vrhs and v_aniso_op now has a
general stress-tensor form,

  grad(A_i*)^T : Pi(V_j)

where A_i is the vector test function.  Various stress tensors may be
inserted for the Pi() function.  In addition to the original kinematic
stress,

  Pi(V)=-rho*kin_visc*grad(V), 

you may use the traceless rate-of-strain,

  Pi(V)=-rho*iso_visc*[grad(V)+grad(V)^T-(2/3)*div(V)], 

where iso_visc is a new viscous diffusivity input parameter.  At this
point, the update for v_rvisc_dot (continuity=='full') and vec_lap_op
are still needed.

  CRS, 9/28/04

  The implementation of the generalized viscous stress with kinematic
and isotropic viscosities is complete, including the update for
v_rvisc_dot.  In addition, the parallel viscous stress has been added
to vrhs and v_aniso_op.

  CRS, 12/02/04

  The parallel viscous stress has also been added to v_rvisc_dot to
make it functional for nonlinear 3D computations.

  CRS, 12/05/04


  nimuw3_2_5

  This version is the merger of the nimuw3_2_4 branch with the main
line.

  CRS, 3/17/05


  nimuw325hmhd branch

  The magnetic field computation has been modified to implicitly
advance the Hall terms simultaneously with the ideal electric field.
The first complete versions will still use predictor/corrector for
center-of-mass flow; though, drift advection from J0 is implicit.
  At this point, modifications for the linear Hall terms are complete,
and nonlinear n=0 only computations should work.  However, we will
need a 3D nonsymmetric solve and possibly a nonlinear algebraic solve
to complete the algorithm.
  Analysis from DISPERSION indicates that this leap-frog scheme is
numerically stable either with predictor/corrector COM flow or with
time-centered implicit flow, but the Hall terms must be time-centered
in the B advance.

  CRS, 3/19/05

  nimuw3_2_6

  The nimuw325hmhd branch is being merged into the main trunk.
However there were some bugs in the branch that have also been
corrected.  The matrix_create call had dirichlet_op instead of
dirichlet_comp_op, iter_solve was called instead of iter_dir_solve,
and the resistive diffusion terms for int(1,3...) and int(2,3...) had
the wrong sign.  These bugs affected n=0 hall computations only.

  CRS, 5/8/05

  There is a correction to brhs_hmhd to ensure that bei is set in all
cases.  The partial(J)/partial(t) electron inertia term has also been
added to b_hall_op.

  CRS, 5/17/05

  nimuw3_2_7

  Timings associated with finite element computations have been
separated for the matrix, rhs, and static condensation calculations.
  The data-type corrections to mpi_allreduce calls for ztr and alpha
in iter_cg_comp have been made--thanks, Alexei Pankin.

  CRS, 7/15/05

  The integrand and int_sum arrays in the matrix-create routines in
rblock.f have been changed to allocatable arrays, and space is
allocated and deallocated during for each call.  This seems to help
avoid exceeding stack memory space with larger jobs.

  CRS, 9/10/05

  Linear gyroviscosity [without the grad(Veq) terms] and the linear
drift heats for separate_pe, +2.5*pi*BXgrad(Ti)/eB**2 -
2.5*pe*BXgrad(Te)/eB**2, have been added.  The input parameter
gyr_visc should be used as a switch: 0 for no gyroviscosity, and 1 for
Braginskii; however, it can be used to scale the magnitude of the
Braginskii gyroviscosity.  The k_cross input is a similar switch such
that the drift heats are used when separate_pe is true and k_cross>0.
Its value also affects the magnitude of these terms with 1 being
Braginskii.  Since these operators are not Hermitian, the adv_v_aniso
management routine calls a non-Hermitian 2D solver when gyr_visc>0,
and the new management routine adv_t_nsym is called when separate_pe
is true and k_cross>0.
  The n-tilde term for the Hall effect in linear computations has also
been coded in brhs_hmhd.  Further scrutiny is needed, however, and it
is presently commented-out.
  At this stage, the development is somewhat preliminary, and the
nonlinear terms have not been coded.  This development was performed
by June, 2005 and some of it has already been added to the nimrodteam
repository (by Dan Barnes), but it's only now being added to the nimuw
repository.  It should also be noted that Alexei Pankin started the
development work for the linear gyroviscous stress tensor.  The
algorithm is the implicit leapfrog described in J. Physics: Conference
Series 16, 2005 (IoP, London, 2005).

  CRS and AYP, 1/5/06

NOTE: Diffusivity shaping with iso_visc>0 and kin_visc=0 does not work
correctly in nimuw3_2_7.  In fact it leads to a numerical instability
because the shaping does not get used in the v_rvisc_dot matrix.  If
diffusivity shaping is needed for isotropic viscosity, set kin_visc to
at least a negligibly small number.


  changes leading to nimuw3_3

  This set of modifications completes the 3D nonlinear implicit
leapfrog algorithm to a pre-production state.  Implicit advection has
been implemented for each of the fields, since predictor/corrector
advection is not stable whenever there is electron drift (see '05 APS
poster--the comment from March above is incorrect).  While the changes
are functioning on very basic test problems, other changes are planned
to improve efficiency and readability.  They include incorporating Dan
Barnes' matrix-free QGMRES solver (the current version here does
matrix-splitting without an accelerator), changing the temperature
advances to the n*dT/dt form regardless of p_model, and separating
advance/management/extrap_init routines for linear, nonlinear hmhd,
and nonlinear p/c mhd (for reproducing old results).
  A detailed list of changes being implemented at present is:

1. The subroutine math_grad has been added to module math_tran in
nimlib.  It works in Fourier-space like math_curl, but it allows an
arbitrary vector-component dimension and an optional symmetric
component.  For 3-vectors, the returned gradient includes the
curvature terms when geom is 'tor'.

2. The extrap_init subroutine and extrap_correct function have been
modified for the implicit leapfrog algorithm.  The conditional
statements are very complicated, and will be simplified by making
algorithm-specific versions.

3. The subroutines in utilities.f have been separated.
Physics-related computations performed at quadrature points are now
located in the new field_comps.f file, while other routines remain in
utilities.f.  There is a new ave_v_check subroutine in field_comps.f
that checks and updates storage for the n=0 part of V, which is used
for implicit advection in 2D matrices.  There is also a no-equilibrium
version of qp_fft_save in utilities.f

4. A new input parameter, mhdadv_alg, allows selecting time-centered
implicit advection ("centered") or predictor/corrector advection
("precor") for ohms="mhd".  If ohms is not "mhd", implicit advection
is used regardless of mhdadv_alg.

5. New quadrature-point storage for the n=0 V, and grad(V) has been
added to the block types, and its initialization appears in
nimrod_init.f with updates in nimrod.f.  The matrix_init routine in
nimrod_init.f has also been changed to accommodate the matrices and
hermitian flags when implicit advection is used.

6. The simple matrix-free solver is located in iter_3d_nh.f.  It will
be replaced by Dan's QGMRES.

7. There are many changes to the management routines in nimrod.f, but
some of them are temporary until the new separated advance/management
coding is complete.  The adv_v_3dn handles 3D matrix-free solves
whether they are hermitian or not.  The adv_t_aniso routine is also
used for all 3D temperature advances, but adv_t_nsym is used for
linear non-hermitian calculations.  For the number density advance,
management is already broken down into the old routine for hermitian
2D solves, linear with implicit advection (adv_nd_nsym), and nonlinear
with implicit advection (adv_nd_3dnsym).

8. Many changes have also been made in integrands.f, since implicit
advection affects all advances.  There is a new b_hmhd_dot routine for
the 3D matrix-free dot-product computation with 2fluid ohms or
implicit advection.  The t_aniso_dot routine now includes implicit
V.grad(dT) and dT*div(V) terms, and the v_rvisc_dot routine now
includes V.grad(dV) and dV.grad(V) terms.  A cont_dot routine has been
added for div(V*dn) for the number density advance.  Corresponding
changes have been made for assembling the matrices used for
preconditioning; they appear in b_hmhd_op (formerly b_hall_op),
t_aniso_op, v_aniso_op, and the new cont_op routine.  The 3D
temperature-dependent resistivity still needs to be added to
brhs_hmhd.

  CRS (with acknowledgements to DCB), 1/22/06

  Handles for the SuperLU C data structures are being modified to
8-byte integers (see the README in externals).  There are also a
couple of minor changes to conditional statements to allow this
version to run with iso_visc/=0 with diffusivity shaping.

  CRS, 2/4/06

  This set of changes is intended to improve efficiency by saving more
time-independent data at the quadrature points.  The newly stored data
is the divergence of equilibrium flow and the equilibrium force vector
that appears in the Hall term with number density variations.  The
b_hmhd_dot routine also makes greater use of the real-space storage
for B+Beq (qbe_tot) and J+Jeq (qja_tot).  There is also one less pass
back and forth to Fourier space when continuity is full.
  In addition, the new math_cadd_cross routines have been used to
reduce the number of saves to temporary storage when cross products
are needed.

  CRS, 3/16/06

  A number of changes have been made to unify the temperature advance
so that it always solves an equation of the form

  n*dT/dt = rhs  .

This eliminates the simpler temperature equation (dT/dt=rhs) that was
used for the adiabat, isothermal, and isotropic p_models.  Specifics
include:

1) Extrap_mod now extrapolates cell-interior temperature data for all
nonlinear computations.

2) The advance routine in nimrod.f has simplified logic for calling
appropriate managment routines for temperature.  At this point
adv_t_iso is not used.

3) The matrix_create routine has been simplified in that a data
structure is always allocated for the temperature advance(s).

4) There are a number of changes to integrands.f.  Conditional
statements in terhs and tirhs that decided whether or not to multiply
or divide terms by number density depending on p_model have been
eliminated.  The multiple return points have also been eliminated.

The aniso0 test option for p_model has also been removed.

  CRS, 5/9/06

  The missing n-tilde*Teq*div(Veq) terms have been added to tirhs and
terhs.

  SEK and CRS, 5/17/06

  The advance subroutine in nimrod.f has been separated into three
versions.  The new advance_il subroutine has time-stepping for
nonlinear implicit leapfrog computations (either Hall-MHD or MHD with
implicit advection).  The new advance_lin has time-stepping for linear
computations only.  It does not have predictor-corrector steps, and
all linear calculations with equilibrium flow will be performed with
implicit advection.  The advance_pc subroutine has time-stepping for
the nonlinear semi-implicit MHD advance with predictor-corrector
advection.  The number of conditional statements in each of these
subroutines is far less than what was required to generate the three
algorithms from the same advance routine.
  Coding for temperature-dependent resistivity has been added to the
integrand routines for Hall-MHD computations.

  CRS, 5/23/06


  pre_3_3_test

  This branch is being used to test centerings and field
representations.  The changes are expected to lead to permanent
additions, but the branch will make it easier to return if necessary.

1.  The centering of n and T has been improved for the implicit
leapfrog.  After n is advanced, the average of its old and new values
are temporarily saved at the quadrature points for the B and T
advances.  After T is advanced, its average is saved for eta(T) and
electron pressure in the magnetic field advance.  Once that part of
the time-step is complete, the quadrature point storage is
over-written with n and T from the end of their updates to provide
centering for the V advance.  The new routines n_store and temp_store
in field_comps.f facilitate the changes.

2.  Pressures in the integrand routines are computed directly from
products of n and T instead of taking products at the nodes and
interpolating.  In addition, the pressure term in vrhs has been
integrated by parts to reduce the number of FFTs.  In brhs_hmhd, the
electron pressure term separates into

  grad(p_e)/ne = k*grad(n)*T_e/ne + k*grad(T_e)/e

The second term is curl-free and has been cancelled analytically.

3.  The p_from_nt routine no longer needs to compute complete nodal
pressures at every step.  It still computes p_tot at the grid vertices
for the nonlinear pressure used in the si operator.  In addition, it
is used to compute nodal pressure before writing dump files; this is
just done for nimplot diagnostics.  At present the pres_n0 field is
just computed from the n=0 part of number density and the n=0 part of
temperature.  This is not the full symmetric pressure perturbation in
3D computations, but the difference will usually be small.  Some of
this could be done at quadrature points instead of at the nodes.

4.  The energy routine in diagnostic_ints has been modified to use n
and T instead of perturbed pressure.

  CRS, 5/24/06

  There was a bug in the integration-by-part for the pressure term in
vrhs.  The R-hat component needed to be multiplied by dalpdrc not
dalpdr.
  The dalpdrc term [ d(alpha)/dr + alpha/r ] for toroidal geometry is
now saved with the other basis data.  This avoids recomputing this
same term many times.
  The t_aniso_op routine has been modified so that the bb dyad is only
needed for anisotropic conduction.  The conditional statements for the
find_bb routine have been modified accordingly.

  CRS, 5/26/06

  The particle diffusivity enhancement for keeping number density
above a floor value has been modified considerably.  The n_store
routine in field_comps.f now checks total n over toroidal angle to
determine if the local diffusivity is below the floor (now set by
nd_floor*ndens).  The extra diffusivity is still based on a hyperbolic
tangent, but nd_exp (*ndens) is used as a width in terms of n for
smoothing the diffusion region.  See input.f for more information on
the input parameters.  Presently, the artificial diffusivity
enhancement is 2D, based on the minimum density over phi.
  Another important modification to this diffusivity is that for the
nonlinear implicit leapfrog, the number density will be recomputed if
the average n from a predictor step falls below the floor.  This
should be safer than leaving the low-n correction for the next
time-step.

  CRS, 5/31/06

  While the n-based particle diffusivity helps, it's effectiveness
seems limited when the profile drops sharply below the floor.  An
upwinding-like diffusivity has been added to compensate.  The
diffusion is anisotropic (parallel to V-total) and there is
effectively a nonlinear coefficient of (dt*V/L_n)**2 multiplied by a
diffusivity of dx**2/dt.  The dimensionless input parameter
nd_dart_upw is used to adjust the magnitude.  The diffusion is
implicit and is only applied in nonlinear cases with implicit
advection.
  NOTE that nd_floor, nd_exp, nd_dart_fac have been moved into the
numerical_input namelist along with the new nd_dart_upw.

  CRS, 6/4/06

  An upwinding-like artificial diffusivity for temperature has also
been implemented.  The user-specified factor is t_dart_upw, and the
diffusion works similarly to the corresponding diffusion for number
density.  At this point, the implementation needs separate
coefficients for separate_pe, but it is functioning for
single-temperature computations.

  CRS, 6/8/06

  The upwinding for separate temperatures has been added, and the
second temperature computation per step has been moved to the end of
the advance, after the magnetic advance.  Average magnetic fields
should now be computed for full nonlinear centering.

  CRS, 6/9/06

  The action of the MHD semi-implicit operator has been restored to
v_rvisc_dot (now renamed back to v_aniso_dot) for the first time since
nimuw3_1.  The cost of computing the extra 2D matrix for the si
operator is too great for cases that need to recompute the matrix
frequently.  The dot routines can now be modified to use the current
symmetric fields instead of the saved "n0" fields, which will allow us
to experiment with recreating matrices and factors less frequently.

  CRS, 6/20/06

  The pre_3_3_test branch has been merged into the main trunk.

  CRS, 6/21/06

  Dan Barnes' GMRES computation has been added to iter_3d_cg.f.

  CRS, 6/23/06

  A number of changes have been made to the upwinding-like
diffusivities for n and T.  Input parameters allow the user to set
limits on the (dt*V.grad(n)/n)**2 and (dt*V.grad(T)/T)**2 factors and
to allow some of the artificial diffusion to be isotropic.
Diffusivities that respond to floor values in n and T as functions of
3D space have also been incorporated into upwinding-like
diffusivities.
  Scott Kruger found a bug in recent changes in tirhs for the div(Veq)
term that would affect computations with viscous heating.  That has
been fixed.
  The v_aniso_dot routine has also been corrected for beta=0
computations, where presr and presz need to be initialized to zero.

  CRS, 7/11/06

  This 1-character change fixes a bug in t_aniso_op that has affected
p_model=isotropic with geom=tor since the modification to unify the
temperature advances to the n*dT/dt form (dated 5/9/06 above but
checked-in on 5/17/06).

  CRS, 7/26/06

  Code for nonlinear gyroviscosity (Braginskii) has been added to
vrhs, v_aniso_dot, and v_aniso_op.  Setting gyr_visc>0 now triggers
the impladv flag, since the matrix is non-Hermitian.  [One should use
the mhd&hall or 2fl Ohm's law, anyway.]  Besides the nonlinear
computation, the extra terms for linear gyroviscosity with equilibrium
flow have been added to vrhs.
  Other changes have been made to facilitate gyroviscosity.  First,
the vcom_store routine (like n_store and temp_store) has been added to
field_comps.f.  Second, the split_visc option has been removed from
linear and nonlinear advances with implicit advection.  Third, there
has been some streamlining of the parallel viscosity computation where
possible, and the gyroviscosity computation in vrhs has been moved
after the viscous heating to avoid numerical heating error.  The
linear parallel viscous term for modulations in |B| with equilibrium
flow has also been corrected.

  CRS, 8/9/06

  Minor changes fix a bug in the conditional statement for the stress
tensor and add insulating boundary conditions as an option for the
temperature advance.

  CRS, 10/20/06


  The nimuw3_3 tag was applied on 1/18/07.


  nimuw3_3_1

*** Note that the default node distribution is being changed to gll.
See the README file in nimlib for more information. ***
  
  The _n0 fields have been modified to make the matrix-free
computations more accurate.  RHS and matrix-free dot-product routines
can use updated symmetric fields, regardless of the matrix; though, it
may impact iterative-solver convergence.  Thus, qbe_n0, qve_n0,
qnd_n0, qpres_n0, and qnl_pres are now updated at every step.  In
addition, what was named qti_sym has been changed to qti_n0, since the
other _n0 fields are now updated every step, too.  The updates for
qbe_n0, qve_n0, qnd_n0, qti_n0, and qte_n0 are preformed in the
respective _store routines.  There is also a new qp0_bcast routine to
provide a generic mpi broadcast call for n=0 data at quadrature
points.
  The update for qpres_n0 is now done from p_from_nt.  The only update
performed by ave_field_check is the one for the nonlinear pressure,
qsi_nl_pres.  The ave_field_check, ave_n_check, and ave_v_check
routines are still used to determine how much the fields have evolved
since the "_changed" flags for updating matrices were last set.  The
node-structure data, like ve_n0(ibl)%arr is now used for saving the
status of the fields at the point where the "_changed" flag was set,
so it is not kept up to date, like the quadrature-point data.  For the
nonlinear pressure, memory for one data point per element is allocated
in the si_nl_pres vector structure, instead of in the rb and tb
structures.  The one data point per element just holds the first
quadrature point of the qsi_nl_pres data at the time that the matrix
update flag was set.
  The new_dt routine in utilities.f has been simplified a bit by using
some of the updated q*_n0 data instead of averaging nodal data.

  CRS, 2/3/07

  The _n0 fields for temperature-dependent resistivity and thermal
conductivity have been modified, like the be_n0, ve_n0, etc. described
above.  In addition, the implicit leapfrog calls the temperature
advance a second time after B is advanced whenever it uses
coefficients that depend on B or T.  The coefficients for the second T
advance are then computed from time-averaged data.

  The matrix create routines in rblock.f and the more intensive
integrand routines for matrices have been modified to improve serial
performance.  Explicit do-loops (and explicit indexing) replace some
array syntax, and temporary scalars are used in favor of temporary
arrays.

  CRS, 2/4/07

  A new character input parameter, norm_flow, is being used as a
switch for calling surface_exb.  Tests run by John O'Bryan show that
using surface_exb (as normally done when there is a tangential
electric field) with continuity leads to an unresolved boundary layer
in number density.  Setting norm_flow to 'none' will skip all
surface_exb calls.

  CRS, 2/10/07

  A number of minor changes have been made to improve free-slip
computations.  First, the new input parameter flow_bc allows a choice
of free-slip or no-slip conditions, independent of viscosity.  Second,
a surface integral for the explicit part of the Lorentz force is
called when free-slip conditions are used.  The surface integral makes
the JxB volumetric force-density calculation equivalent to using the
Maxwell stress tensor, integrated by parts without a surface term.  So
far, the integrand routine, mag_tension, only uses linear terms.
Nonetheless, it requires interpolations, so there are new routines
(generic_2D_eval and generic_3D_eval) in generic_evals.f.
  Some experimentation with Lobatto integration did not lead to
improved performance, but the capability is general enough to leave in
the code.  Thus, the input parameter integration_formula is passed to
the rblock and surface set-up routines.
  Finally, there is a bug fix in nimrod_init for the relatively new
initialization of qti_n0 and qte_n0 structures.

  CRS, 2/21/07


  nimuw 3_3_2

  There are a number of non-major changes at this time:

1.  The check for changes in velocity has been too restrictive when
determining whether matrices for preconditioning with implicit advection
need to be recomputed.  It is commented-out for now, at least.

2.  The 'diagonal' preconditioning routines in iter_cg_f90.f and
iter_cg_comp.f have been modified to fix an old bug that affects
poly_degree>1.  A 'no prec' option has also been added that skips
preconditioning entirely; this helps test the iterative algorithms.
The routines that communicate matrix elements across block boundaries
have been modified for non-symmetric matrices.

3.  The iter_dir_nonsym module now contains GMRES iterations instead of
simply iterating the preconditioning routine.  This is for 2D algebraic
systems, where we typically use SuperLU to provide a direct solve, but
this change allows us to rely on iteration with other preconditioners
if desired.

4.  The GMRES computation for 3D algebraic systems in iter_3d_cg.f has
been modified to generate the Hessenberg matrix and then perform the
minimization computation after finding enough bases to meet the
tolerance for the residual.  This reduces the number of updates to the
solution vector itself.  The algorithm is described in Saad's book.
However, the quasi part is temporarily disabled.  The GMRES operation
uses a maximum of maxit iterations.

5.  An option to use homogenous Dirichlet boundary conditions for the
number density advance has been added for nd_diff>0.  A new input
parameter, nd_bc, determines whether natural or essential conditions
are used.

  CRS, 6/14/07

  More non-major changes:

1.  Checks for the processor-node number have been added to the
diagnostic writes in iter_3d_cg.f and iter_dir_nonsym.f.

2.  There are additional checks on impladv before computing any of
the 'upwinding-like' diffusion coefficients.

3.  The partial(J)/partial(t) term for electron inertia is being
added to b_hmhd_dot.  The advective-like term is still in limbo,
however.

4.  The _store operations have been rearranged to give temporally
consistent energy computations after a time-step.  The nim_ouput
call at the start of a computation has been removed, because
consistent information is not available.

  CRS, 7/3/07

  The gfortran compiler (or optimizer) gets confused when the
transpose operation appears immediately inside a conjugate operation.
Reversing the order seems to be fine, however.  Iter_cg_comp.f has
been modified to avoid the problem.

  CRS, 7/4/07

  Two bug fixes: an imode do-loop for linear computations was missing
for the stress tensor accumulation in vrhs.  Also, iter_dir_nonsym.f
needed an update for the cvector_2D_addc_cvec2 routine that was added
to vector_type_mod.f in nimlib.

  CRS, 7/9/07


  nimuw3_3_3

  The primary changes here correct the temporal centering of the
viscous heating computation.  Its density had been computed in vrhs
to reuse the stress tensor, but the velocity is off by a full time-
step, and the number density and magnetic field are off by a half
time-step, relative to the temperature advance.  Improving this
requires separate stress computations (except gyroviscosity) for
the viscous heating alone.  To reuse coding, grid-block-level
computations for the kinematic, isotropic, and parallel stresses
have been moved from vrhs to new external subroutines in
field_comps.f.  The viscous_heating subroutine has also been added
to field_comps, and it is called from the top-level advance routines
before the temperature advance (and again before the temperature
correction in the implicit leapfrog).
  Along with the above changes, stresses from equilibrium flow
have also been moved out of vrhs and are now computed only once 
during start-up.
  The computation of momentum-density advection has also been
modified to make use of the real-space grad(V) tensor that has been
used for stress computations.  In addition, full-continuity
calculations multiply by number density right away before the
inverse FFT to reduce the number of FFTs (and possibly sacrificing
some dealiasing; it's a cubic nonlinearity).

  CRS, 7/11/07

  A vector gravitational acceleration has been added to vrhs (explicit
only at this point).

  CRS, 7/31/07

  The nonlinear diffusivities have not been getting initialized
correctly, because they depend on dt, which has not been set the first
time the store operations are called.  Changes in the initialization
sequence for calling soln_save and store operations, together with
some modifications to new_dt correct this.  Another bug in
quadrature_save had the qp0_bcast calls for temperature in the wrong
place.
  A bug in adv_t_nsym left the logic for insulating boundary conditions
backwards, so that Dirichlet conditions were applied when insulate
was true.
  A modification to diagnostic_ints.f changes the computation for
internal energy, so that the n-tilde*T-tilde product is included
for linear computations.
  Code development (finally): modifications for temperature-dependent
parallel viscosity have been added.  It uses the same routines and
data structures that were created for temperature-dependent ion
thermal conductivity.  The dependence is the same, and the
required coding is minimized.

  CSC, RAB, and CRS, 8/8/07

  nimuw3_3_4

  Improvements to the linear solvers are being checked-in at this time.
In all four of the solvers (iter_cg_f90, iter_cg_comp, iter_dir_nonsym,
and iter_3d_cg), the initialization now averages the provided guess
along block borders, which should have duplicate images.  Averaging
prevents an error, where the extrapolation for the guess gradually
leads to a significant drift.  It is not known why this is showing up
now but not previously.
  Also, the iter_3d_cg and iter_dir_nonsym routines now save a second
set of vectors that are the basis vectors with the preconditioning
operation applied.  This helps preserve accuracy when the
preconditioning matrix itself is ill-conditioned (due to very large dt).
  The nimuw3_3_4 tag is NOT being applied at this time.

   CRS, 1/6/08

  Modifications to use the distributed-memory interface for SuperLU-DIST
are being added at this time.  The changes are intended to use less
memory when running in parallel at the possible expense of some CPU
time.  A more thorough description is provided in the change-log
comments in iter_cg_comp/f90.  Note that the new interface is invoked
with the solver parameter set to 'slu_dstm' and that the full-storage
interface is still available with 'slu_dist.'  However, communication
on the nimrod side has been improved for the old interface, and it
also uses less memory.  The sequential verion, 'seq_slu,' has also been
modified to use compressed-row storage (like slu_dstm), which is more
efficient on the nimrod side.

  CRS, 1/7/08


  nimuw3_3_5

  The vrhs routine had an incorrect index coded when computing gyro-
viscous stress with equilibrium flow.

  CRS, 3/27/08

  The -2*Vphi*dVphi/r implicit flow contribution to v_aniso_op has
not been multiplied by the scalar test function due to a misplaced
close-parenthesis.  This has been corrected, and more explanatory
commenting has been added.

  CRS, 4/27/08

  An option to skip dealiasing and use Fourier components up to
Nmax=2**lphi/2 has been merged from the slice preconditioning
branch.  The changes are very minor but they affect all calls to
fft_nim.
  Other changes taken from the slice_from_334 branch include
the vector_3D_type and matrix seaming for nonsymmetric real
systems in iter_cg_f90.f.

  CRS, 6/8/08

  The real_matrix_create and comp_matrix_create routines in the
finite_element_mod module determine a diagonal scaling factor for
the respective dirichlet_op routine.  The passed factor multiplies
the diagonal matrix element (or n-hat n-hat dyad) that is inserted
after couplings are eliminated for essential conditions.  This
avoids finite-precision inaccuracy, which causes discrepancies
between the iterate residual and the actual residual, in iterative
solves.

  CRS, 6/9/08

  The iter_3d_cg_m2v routine had been coded for 3-vectors only.  It
has been modified so that nqty is a passed parameter.

  CRS, 6/10/08


  nimuw3_3_6

  The explicit advective force for linear computations now includes
the rho_tilde*V0.grad(V0) term.  Chris Carey found that this is
important in computations of rotational stabilization of kink modes.
[Nonlinear computations with full continuity already included this
effect.]

  CRS and CSC, 7/1/08

  Eric Howell found that the revised energy density computation in
nimuw3_3_5 diagnostic_ints did not consider cases with
continuity='none.'  

  CRS and ECH, 7/11/08

  A 4-th order in space hyper-diffusivity for number density has been
added using an auxiliary scalar to split the operator into two
2nd-order parts.  It has been implemented for linear and nonlinear
computations that use implicit advection (ohms/=mhd or mhdadv_alg=
centered).
  The modification requires new work arrays.  [The old neo_ data has
been scrapped while doing this.]  The ndrhs, cont_dot, and cont_op
integrand routines have additions for the auxiliary scalar and its
contributes to the continuity equation.  The adv_nd_nsym and 
adv_nd_3dnsym management routines have been modified to use 2-vector
systems when solving n and the auxiliary field simultaneously.  Finally,
the regular_vec routine has a very minor change to handle a system
of two scalars.

  CRS, 8/7/08


  merge from fourgs_from_335nd branch (part of nimuw3_3_6)

  This branch contains modifications for a preconditioning step that
applies Jacobi or Gauss-Seidel type iteration including limited coupling
among Fourier components.  To be more efficient than the polynomial
preconditioning, where FFTs are used, the matrix elements for the off-
diagonal components are computed and applied as 2D nimrod matrices.  
They are stored in standard arrays of complex_matrix_type structures.
A new component of the matrix type is the foff integer, which indicates
the Fourier-index offset for the column relative to each row index.
The structures themselves are computed through matrix_create routines
in finite_element.f, and while a separate integrand routine is needed,
it is readily generated from the respective diagonal-in-Fourier-index
integrand routines that has been used for preconditioning.
  So far, only the adv_b_3dnsym advance has storage and an integrand
routine for the off-diagonal matrix elements.

  A detailed list of changes for the new preconditioning is:

1) bhmhd_plus and bhmhd_minus data structures are defined.
2) The routines for applying essential conditions in operators in
   the boundary module have been modified for off-diagonal data
   structures.  for these data structures, there are no diagonal entries
   after eliminating couplings.
3) The regularity module has also been modified to be consistent with
   the row and column Fourier indices being different.
4) There is a new qpc_bcast routine for broadcasting specific Fourier
   components of a field for use in computing the integrand for off-
   diagonal matrix entries.  it is similar to the qp0_bcast routine
   that was in field_comps.f, but both have been moved to parallel.f.
5) The global module now has an nindex_total array that has the
   integer Fourier index for all Fourier components, independent of
   layer decomposition.
6) The b_hmhd_cpop integrand routine has been added to integrands.f
   for generating the off-diagonal matrix elements.  the b_hmhd_op
   routine has also been optimized.
7) The iter_3d_cg.f routine has been modified to accept and use the
   off-diagonal-in-Fourier-index matrix structures for use in Jacobi
   or Gauss-Seidel based relaxation.  Most of the modifications appear
   in the iter_3d_cg_pre routine, and these relaxation passes appear
   within the polynomial-approximation iteration loop.  There are new
   routines iter_3d_offload, iter_3d_offunld, and iter_3d_offrready
   that are used to load and unload communication buffers when parallel
   layer decomposition is used.  The Jacobi iteration is independent
   of nlayers, but the Gauss-Seidel iteration uses the current iterate
   only within each layer.  Either one may use a relaxation factor.
   The communication is asynchronous point-to-point and is written to
   overlap communication and computation.
8) A stub for mpi_wait has been added to mpi_serial.f, because the
   real mpi_wait is used in the new preconditioner.
9) adv_b_3dnsym has been modified to create the off-diagonal matrix
   elements when needed.
10) nimrod_init allocates the new data structures.
11) As noted above, qp0_bcast and qpc_bcast have been added to
    parallel.f.
12) A new array, mode2layer, has been added to pardata.
13) In input.f, four new input parameters (nsym_pre_band,
    nsym_pre_rpass, nsym_pre_rfac, and nsym_pre_rtype) are defined to
    control the use of the new preconditioner strategy.

  CRS, 7/20/08

  Couplings associated with number density perturbations have been
added to the Hall operator for continuity=full nonlinear computations.
The changes are primarily to the b_hmhd_cpop routine, but qpc_bcast
calls for number density have also been added to n_store in
field_comps.  The number density perturbations within the specified
band are used in binomial approximations for the derivatives, which
assumes that the non-symmetric number density perturbations are much
smaller than the average.
  The iter_3d_cg.f file is also being modified to avoid use of
optional pointer arguments, which the Portland Group compiler does
not like.

  CRS, 8/7/08

  The merge of the fourgs_from_335nd branch (up to fourgs_nfz) is
being made at this point.  There are two additional minor changes.
The bplus matrix is not computed if nsym_pre_rpass is 1, where
only the bminus matrix is used in the Fourier-off-diagonal
preconditioning.  Also, setting nd_hypd>0 automatically triggers
the impladv switch.
  Note that second-order terms wrt Fourier coupling were coded
and commented-out of the b_hmhd_op in the fourgs_from_335nd branch.
These commented-out lines have been removed.

  CRS, 10/1/08


  qpreord_from_335hypd branch

  This branch tests a reordering of data and numerical quadrature loops
to improve parallel efficiency and optimization.  The quadrature-point
storage now has the second array index as the index for different
integration points within an element, and the third index is a unified
element index.  The arrays within the rblock and tblock data
structures (lowest level arrays are qpf, qpfr, qpfz) each have one fewer
array index.  Routines for interpolating quadrature-point data, and
the generic_evals routines for finding the data have been modified
accordingly.  Changes to the field_comps routines take advantage of
the new data ordering.
  The second set of these changes (post 9/22/08) puts the summation
over quadrature-points within the integrand routines.  For rhs
computations, the factor of the quadrature weight times the Jacobian
is multiplied into the test functions, so that it does not appear
explicitly.  For matrix computations, the square root of this factor
is in the test and basis functions, so that their product is
complete with respect to weighting.
  This second set of changes reduces temporary array storage and the
amount of data in cache, but the cost is a change to each integrand
routine to sum contributions.  There are also changes to the rblock
and tblock routines (get_rhs and make_matrix), since they no longer
perform the numerical quadrature.  Their primary purpose is now just
scattering element contributions to each test/basis function index
in the global matrix and rhs arrays.

  CRS, 11/03/08


  nimuw3_3_8  (3_3_6->3_3_7 has no changes in kernel trunk)

  Computing Jeq from Beq at the quadrature points has been added
as an option to nimrod_init.f.  It uses the logical input parameter
from nimdevel, tor_eqja_fe, and it will now work for linear geometry.
  Multiplying p at the nodes for the pressure force in vrhs, as
was done prior to nimuw3_3, is now a default option.  This patches
a growing noise error in some toroidal computations.  The multiplication
can also be done at the quadrature points, 3_3-style, by setting the
new p_computation parameter to 'at quads'.  The p_from_nt routine in
field_comps.f has been modified slightly to use fewer FFT calls in
nonlinear computations with the default option.

  CRS, 2/18/09  

  In nimrod_init.f, the qnd data structure is now allocated independent
of the continuity setting.  Also, Scott Kruger and Eric Held identified
a bug in fft_mod where there are multiple layers but each layer has the
entire block (code not typically used).  The n=0 component was getting
zeroed-out in forward transforms when it should be zeroing-out just the
imaginary part.

  CRS, 2/23/09

  The adv_b_3dnsym and adv_v_3dn management routines have been modified
to allow iteration for time-centering the nonlinear parts of the Hall
electric field and the momentum-density advection.  Both are quadratic
nonlinearities for our algebraic equations with the staggered temporal
centering.
  The solution of the algebraic equations is always the change
(delta-B / -V) from the beginning of the time-split, i.e. it is not
just the change from the last nonlinear iteration.  Coefficients of
the linearized terms in the dot routines are updated with each non-
linear iteration.  The correction on the rhs then has the opposite
sign of the explicit term.  For example, when solving for the i+1th
nonlinear iterate of the change in velocity (and dropping all but
advection), we have

 dV_i+1 + 0.5*dt*[(Vn+0.5*dV_i).grad(dV_i+1)+dV_i+1.grad(Vn+0.5*dV_i)]
        = -dt*Vn.grad(Vn) + 0.25*dt*dV_i.grad(dV_i)

where Vn is the velocity at the start of the step.  As the nonlinear
iterations converge (dV_i+1=dV_i), this is equivalent to

 dV_i+1 = -0.5*dt*[Vn.grad(dV_i+1)+dV_i+1.grad(Vn+0.5*dV_i+1)]
          -dt*Vn.grad(Vn)
        = -dt*(Vn+0.5*dV_i+1).grad(Vn+0.5*dV_i+1)

This organization minimizes changes to the code.
  There are new integrand routines that compute just the nonlinear
correction to the rhs, and store routines have new options to
save the old field plus the last iterate (for the coefficients
of the matrix-vector 'dot' routines).  Other changes are the
new input parameters maxit_nl and tol_nl that control the maximum
number of nonlinear iterations and the nonlinear tolerance,
respectively.  The iter_3d_err routine has also been modified
slightly to accommodate calls from outside the the 3d linear solver.
It is used to find the norms for checking nonlinear tolerance.

  An unrelated bug fix is that the computation pointers are now
dealocated after a linear solve fails to converge.

  CRS, 3/13/09

  There is another change related to convergence-failure recovery.
In multi-layer computations with solves that do not couple Fourier
components, some layers may have a converged=false results while others
do not.  This is being corrected with extra mpi_allreduce calls.

  CRS, 4/3/09

  Minor fixes include a catch for trivial rhs arrays in iter_3d_cg,
elimination of an extraneous loop in soln_save, and correcting the
matrix-compute conditional logic in adv_nd.

  CRS, 6/16/09

  
  nimuw3_3_9

  The e_tangential surface integrand now adds kboltz*grad(Te)/e to
the electric field in non-MHD computations.  This electrostatic
term is added to E in the interior, and the surface term should
be consistent with the interior.

  CRS, 7/13/09

  There are two changes to integrands.f and a bug fix in utilities.f.
The tirhs and terhs now use explicit looping for the Ohmic heating
computations with jaeq2.  This is an old work-around for the PGI
compiler (found by Vicki Lynch at ORNL) that hadn't been installed in
the main trunk of nimuw until now.
  In v_aniso_op, the 1/R-terms for isotropic viscosity for
toroidal geometry have been incorrectly added in linear-
geometry cases.  This has only affected geom='lin' computations
with iso_visc/=0, and for nonlinear computations with full continuity,
it would just slow the linear-solver convergence rate.
  The correction in utilities.f is that factors of 1/mu0 were missing
in the 'nonlinear' CFL computation.

  CRS, 8/20/09

  A new option for eta_model, "chodura," has been added.  It adds the
phenomenological Chodura model to Spitzer resistivity, and this
new component increases with electron drift, |J/ne| relative to
the ion acoustic speed.  The model uses 3D, time-dependent J, n,
and T.  See input.f for more details.
  Also, there is a new parameter, elecd_min, for setting a minimum value
for Spitzer resistivity.

  CRS, 8/28/09

  The block_create_tang routine in nimrod_init.f has been modified
slightly to accommodate new capability in stitch.  A 3-block concave
corner may be formed, and the corner block will have an isolated
exernal seam vertex.

  CRS, 9/24/09

  nimuw3_3_10

  A set of parentheses were missing in the grad(Te) X n_hat part of
e_tangential in surface_ints.f.

  CRS, 1/13/10

  nimuw3_3_11

  There are three corrections here:

1) There was a bug in v_aniso_op in integrands.f.  The local variable
dvas should have been declared as complex, but it has been a real.  This
bug was introduced with version nimuw3_3_1 (2007) and affects the
isotropic viscosity operator and implicit advection.

2) The electron pressure term in the two-fluid Ohm's law incorrectly
included n_tilde*grad(Teq)/n**2 terms.  Like the grad(Te_tilde) term,
this should be eliminated from the curl of E before expanding perturbed
parts.  The correction is in nimrod_init for qeq_force.  (Thanks to
Eric Howell for identifying the error.)

3) The numerical factors of kboltz/elementary_q have been included in
gyroviscous contributions.  This helps avoid errors when users normalize
the physical constants.

  CRS and ECH, 8/31/10

  nimuw3_4

  This version is the merge of the qpreord_from_335hypd branch into the 
main trunk.  It is equivalent to qpreord_at_1_1_11, which updated the
qp branch to nimuw3_3_11.

  CRS, 1/7/11

  from the newred_from34 branch:

  The branch has more flexibility with respect to polynomial degree.
Although some of the changes from this branch are not kept for the
main trunk of nimrod, changes that allow discontinuous fields are.
  For the physics kernel, nimrod, the primary changes are:

1) The rblock data structure now allows different sets of test/basis
   functions for the different expansions.  The number of these
   expansions depends on the input, and rblock_basis_set will decide
   how many distinct expansions are needed.

2) When the integrand routines call the generic_alpha_eval routine,
   they specify the appropriate polynomial degree for that equation.

3) The generic mass matrix is only used to create the j_mat for jfromb
   if that is used.  All operator routines in integrands now have
   some form of the mass matrix appearing there.  Also, lumping has
   been disabled.

  CRS, 3/9/11

  nimuw3_4_1

  The block_create_tang routine is getting 'fixed' again.  The
vectors that are averaged across grid vertices should all be
normalized surface tangents before and after summing.  Averaging
the tangent basis vectors (derivatives of rz data structure)
is not correct, as it will get biased to the larger of the two
adjacent elements at each grid vertex.

  CRS, 6/3/11


  from the newred_from34 branch:

  Routines have been modified to find a field that is discontinuous
at element borders simultaneously with a continuous field.  The nodes
of the discontinuous fields are GLL (or uniform; modal added later)
with some at element  borders.  This is nonstandard for
spectral elements but may be useful for interface computations.  The
first application is to solve a discontinuous scalar div(b) field
in the B-advance for divergence control.  Modifications are:

1) matrix_type_mod now has degree-of-freedom (DOF) arrays that record
vector indices and basis indices in the element-based storage (integrand
output) and quantity indices and ix and iy offsets for the rblock
storage.

2) The make_matrix routines have been modified to use the DOF
information and eliminate the small offset loops that would have been
too complicated with the discontinuous fields.  Also, the
get_comp_rhs_q routine accumulates discontinuous-basis information in
the new arrtmp array in the passed rhs structure.

3) The finite_element module has been modified to combine element
'interior'- and 'discontinuous'-field data for the matelim presolve
and postsolve computations.  Like interior coefficients, all
coefficients for discontinuous fields are eliminated prior to calling
matrix-solve routines.

4) The integrand routines brhs_mhd, brhs_hmhd, and brhs_hmhd have
the extra terms of +SQRT(fdivb*disc_dbd*dt)*div(A^*)*phi in the LHS
of the B advance for linear implicit advances (from adv_b_nsym).  The
terms also define the auxiliary equation

  int[ (chi^*)*phi-SQRT(fdivb*disc_dbd*dt)*div(dB)*(chi^*) ]
    =int[ SQRT(disc_dbd*dt/fdivb)*div(B_old)*(chi^*) ]

5) In nimrod, impladv is also flagged when poly_divb>=0, and the
adv_b_nsym manages the setup with the div(b) scalar.  Note that at
this time, this div(b) control is only available for linear
computations.

6) generic_evals will take information from discontinuous bases if
the flag in the generic_alpha_eval call starts with 'dis.'

7) The polynomial degree can no longer be inferred by summing nb_type
in rblock matrix structures.  The degree for continuous fields can
be obtained by summing nb_type(1:2), however, and this modification 
has been made to matrix_mod, boundary, iter_cg_comp, iter_cg_f90,
and iter_dir_nonsym.

  CRS, 10/22/11


  from the newred_from34 branch:

  The following set of changes adds capability for solving strictly
discontinuous fields.

1) In matrix_type_mod, when allocates are called with 0 for the
number of continuous components, nbtype is still set to 4, but
only the 4th is used.  nb_type(1:3) and nq_type(1:3) are set to 0.

2) In matrix_mod, the mat_inv_int and presolve routines can be used
for interior-only computations.  Computations for edge and
corner bases are skipped if their quantity flags are 0.

3) rblock updates check if the %fs array is allocated and uses this
to decide whether to call lagr_quad_eval or lagr_disc_eval.  The
generic_evals routines use the same condition.

4) dump also checks %fs before writing lagr_quad_types.  If it is not
allocated, -1*nqty is written in the nqty spot to flag a
discontinuous field, so that only fsi will be read.  Note that
parallel_io has NOT been updated at this point.

5) In the finite_element make_matrix routines, regular_op and
boundary routines are skipped if there are no continuous fields.  In
get_rhs, there are more checks on what is allocated in the rhsdum
structure.  If there are no continuous fields, only rhsdum%arri is
allocated and used.  If there are both continuous and discontinuous
fields, rhsdum%arrtmp is used for the discontinuous fields.  Note
that if there are no continuous fields, the presolve call solves the
element-local system.

  CRS, 10/31/11


  nimuw3_4_2

  Modifications from working with DDS and ECH on ITG-like modes and
using the diamagnetic heat flux are being added.  First, the 
variable_alloc routine puts a lower bound of smallnum for the
nodal storage for tele_eq and tion_eq.  The quadrature_save does
the same for quad-point storage of these fields.  This prevents
NaNs when using separate_pe and the diamagnetic heat flux with
pe_frac in going to the limit of 0 or 1.  Second, the
factors for the diamagnetic heat flux have k_boltz/q_elementary
appearing explicitly in case these constants are changed for
normalized units.  Third, in terhs and tirhs, the b-tilde part
of the diamagnetic heat flux has been added.  Fourth, the 
t_tilde/T_eq part of the diamagnetic heat flux is being added
to the implicit operator from t_aniso_op.
  There are also minor modifications to iter_cg_comp.f and
iter_cg_f90.f with respect to convergence checks.  The code does
not need to restart when the actual error meets the specified
tolerance.

  CRS, 5/14/12

  ITG-related tests run by DDS pointed to a problem with the
separate Te-advance option.  To be consistent with implicit
leapfrog, the J used in the Te advance needs to be centered in
time, even with ohms=MHD.  A corrector step is already used
for nonlinear computations with anisotropic thermal conduction,
and a similar step is being added to advance_lin for linear
computations.  Also, the impladv flag is now getting set to
true for linear computations with separate_pe.

  CRS, 5/17/12

  Thermal conduction models that incorporate magnetization effects
at low temperature and/or low |B| have been added.  They are used
when p_model is set to "aniso_tdep" in which case, the new input
closure_model is used to specify a conduction model.  The default,
"std kperp n0" is the old model for high-magnetization with
perpendicular conduction based on symmetric fields only.  Other
options use fully 3D perpendicular thermal conductivity:

  "standard" is the old high-magnetization formulation but with 3D 
             perpendicular conductivity.
  "braginskii" is the Braginskii model with temperature- and field-
             dependent magnetization evaluated locally.
  "k2" is Ji's k2 model, also with fully 3D magnetization.

Other new input specifies whether the Coulomb log should be
a prescribed value or locally dependent on temperature and density.
  Thermal equilibration has also been revised to allow temperature-
dependent equilibration rates in nonlinear runs.  The computation
also invokes the Coulomb log option.
  NOTE: if temperature-dependent equilibration is used, it drives
perturbed plus equilibrium temperatures to the same value, unlike
the temperature-independent model, which just equilibrates perturbed
temperatures.
  There is also a bug fix in the temperature-independent equilibration
model.  The input tequil_rate had been used as an equilibration time
and not a rate.  It is now used as a rate.

  JBO and CRS, 8/10/12


  from the newred_from34 branch:

  Stabilization for flow divergence and for parallel
vorticity have been added using new incomplete modal expansions
for discontinuous auxiliary fields.  Their expansions are stored
if they are used, and the bases are specified through the input
parameters poly_divv, poly_divv_min, poly_pvrt, and poly_pvrt_min.
See input.f for more information.

  CRS, 6/20/13

  The expansions for stabilizing V have been futher generalized
to allow lower and upper limits that differ from the degree for
the coordinate with a complete expansion.

  CRS, 6/21/13

  The auxiliary fields have been modified in two ways.  First,
the expansions for the div(V) and B.curl(V) auxiliary fields now
have the same bases, so poly_pvrt and min/max are no longer used
as input.  The stored auxiliary data is now together as a single
2-vector, auxv.  These two changes simplify management coding
and make the matrix-create routines for V more efficient memory-
wise.
  The auxiliary discontinuous scalar for div(B) has been changed
to a modal expansion, and it may now be incomplete.

  CRS, 7/17/13

  This set of changes allows application of the V-stabilization to
nonlinear computations.  The significant changes are the following:

1) The v_aniso_dot routine in integrands now includes the effect of
the auxiliary fields when finding matrix/direction-vector dot
products during 3D solves.  The mwork arrays hold the modal
expansion for the new direction vector.

2) The flow-divergence stabilization now includes a factor of
the square of the magneto-acoustic speed so that the ddivv
coefficient is normalized, like the dpvrt coefficient
for stabilizing parallel vorticity.

3) The GMRES algorithm in iter_3d_cg now includes coefficients from
auxiliary discontinuous fields.  The corresponding rhs coefficients
are passed in the arrtmp part of the vector structure for the rhs.
There are also two new integers, nqdsc and nbdsc, that are passed into
iter_3d_ky_solve to specify, respectively, the size of the quantity
dimension and the number of bases for the discontinuous fields.

4) The get_rhs and fe_comp_postsolve routines have been modified to
take advantage of the new vector-type packing routines where static
condensation operations include coefficients for discontinuous bases.

  CRS, 8/19/13
 

  nimuw3_4_3

  The only changes from the 3_4_2 version are in the solver/
preconditioner routines.  Jacob King's slu_dsta version of the
distributed-memory SLU interface has been installed.  It uses point-
to-point communication to create the sparsity pattern, avoiding the
increasing memory requirements with parallel scaling that occcurs
with slu_dstm's use of mpi_allgatherv.  Information on the sparsity
pattern is now contained in the new spp data structure within
matrix-factor structures, and the initialization operations have
been unified in the new iter_dir_fac module in iter_utils.f.
  The old ilu preconditions options have not been used and are
removed with this version.

  JRK and CRS, 8/26/13


  nimuw3_4_4

  This version merges the discontinous-field development,
V-stabilization, and discontinuous div(B) correction from the
newred_from34 branch into the main trunk.  Comments on the
branch appearing above do not describe other development that
is not being merged at this time.
  An unrelated change is that the option to pass a 2D matrix
into the 3D solver for the purpose of minimizing matrix-free
dot-product routines has been removed.  The option was not
related to preconditioning, and it hasn't been used for some
time.

  CRS, 9/5/13


  nimuw3_4_5

 Iter_3d_cg has a correction to avoid segmentation error in 
vector-field assignment when the algebraic system has no
discontinuous bases.  Previously, the corresponding
(discontinuous-coefficient) arrays would be allocated with
nq=0, but they would get assigned values from a structure where the
discontinuous arrays were nullified.  Now, if nqdsc is 0, the solver
arrays for the discontinuous bases are also nullified.

 CRS, 2/27/14

  Linear computations by Eric Howell identified a bug in t_aniso_op
for the temperature advance with implicit advection and n/=0.  The
nvs array is a real, but a complex number was added to its third
vector component, just before the basis loops at the end of the
routine.  The complex information is needed for V_phi*grad, so
a new complex array is now used instead of nvs(3,:,:).  The bug
made linear, centered-MHD and two-fluid computations numerically
unstable for n/=0, and it will hinder preconditioning for nonlinear
computations.

  ECH and CRS, 6/30/14

  The boundary module for essential conditions has a new option
for the rhs and the real matrix routines.  It is a logical and
optional symmetry flag that is used by nimeq for vertically
symmetric equilibria.  Besides the modification to boundary,
interface blocks in finite_element and iter_3d_cg.f are updated.

  KJB and CRS, 7/20/14


  nimuw3_4_6

  merging from hypeta_from345 branch:

  This branch includes a hyper-resistivity term with a numerical
uniform hyper-resitivity coefficient, which is specified in
units of length**4/time.  The initial implementation is in
brhs_mhd, brhs_hmhd and b_hmhd_op for linear computations.

  CRS, 10/7/14

  The next installment of the hyper-resistivity allows a split
hyper-diffusion advance for nonlinear computations.  There are
new integrand routines that compute the rhs and operator for
the split step.  The adv_b_hyp management routine handles the
6-vector computation for the unsplit linear advance and for
the hyper-resistive nonlinear time-split.
  Regularity routines have also been generalized for multiple
3-vectors.  However, there may be other systems where the
number of components is a multiple of 3, and the present coding
will apply vector regularity conditions without careful
checking of what is or is not a 3-vector.

  CRS, 10/10/14

  The hyper-resistivity is now more general allowing split or
unsplit advances for both linear and nonlinear computations.  The
split version also has a correction for all multi-mode runs.

  CRS, 10/12/14

  A hyper divergence cleaning operator has been added to all
places where the hyper-resistivity operator is programmed.  It
has separate diffusivity coefficient and centering parameters.
  There was also a sign error in the int(5,3,...) computation
of the 2D operator, and it has been fixed.

  CRS, 10/18/14

  nimuw3_4_7

  The n=0 part of pressure that is used in the flow-stabilization
term did not have a pointer set before being added to the divergence
coefficient.  This was corrected on 2/28/15.
  The dump read and write routines now have the dump file name
passed to be consistent with changes in the vblock branch.

  CRS, 7/27/15

  nimuw3_4_8

  The 3d solver interface is getting updated to improve
modularity.  Like the vblock branch, a dot-product management
routine is passed in place of separate integrand routines.  The
new threed_dot_mgt.f file has the dot-product routines.  Also,
the iter_3d_ky solves no longer use input parameters through
the finite_element module.  With this and the external direction-
vector computation, the iter_3d_cg module does not depend on
the f_e module at all.
  The matrix initialization for the bplus and bminus matrices
now dimensions those structures for 6-vectors when hyper
resistivity or div cleaning are used.  The off-diagonal solver
passes may now be used with 6-vectors b solves, although
the algebra in those passes multiplies a lot of zeros.
  The external subprograms that have been in the iter_cg.f
file have been moved to the new iter_externals.f file.  Two
of those subprograms access the fields module, and having
them together with the iter_cg module tangles solver and
physical model dependencies.

  CRS, 8/8/15

  nimuw3_4_9

  Subsequent to the migration to svn, the organization of
directories has been revised to improve modularity and to conform
more with nimdevel.  Many files have been moved out of the
nimrod directory and into the nimcore directory, and compiling
creates a collection of new libraries in nimcore.  The files
that have been moved out of the nimrod directory are

  matrix_type_mod.f
  matrix_mod.f
  mpi_serial.f (moved to nimlib not nimcore)
  mpi_parallel.f (moved to nimlib not nimcore)
  parallel.f
  parallel_io.f
  pardata.f
  *block.f
  fft_mod.f
  boundary.f
  regularity.f
  generic_evals.f
  computation_pointers.f
  time.f
  iter*.f

  Also, nmodes and nindex are now passed into the regularity
subprograms to make the regularity module independent of global.
The broadcast_input subroutine has been separated from the rest
of parallel.f and placed in the parbcast_input.f file that remains
in the nimrod directory.  The subprograms in parallel.f in nimcore
are now independent of the input module.

  CRS, 9/8/15

  nimuw3_4_10

  The 3D semi-implicit operator has been recovered from development
in 2009.  It has been installed as an option, unlike the earlier
development.  There is a new input parameter siop_type to select
from the 'standard' (2D) operator or the '3D' operator.  The 3D
operator saves more quad-point data before the V-advance, and it
uses its own dot-product integrand, which is v_3dsi_dot.

  CRS, 12/25/15

  Reinstalling the 3D si operator uncovered a couple of problems
that are now fixed:
  1) Newton iteration for V.grad(V) and for JxB did not update the
right-side vector correctly when using discontinuous auxiliary fields
for the V-stabilization terms and for the poly_divb divergence
correction.  The computations of nonlinear residuals and their norms
now use just the coefficients for the continuous fields, leaving
coefficients for the discontinuous fields alone.
  2) In adv_b_3dnsym, the old B vector passed to the 3D solver was
not dimensioned correctly for unsplit hyper-resistivity or hyper-
divergence cleaning.  A separate 6-vector is now used and has been
tested with and without the poly_divb option.

  CRS, 12/27/15

  The handling of essential conditions during matrix-free Krylov
solves is being modified to ensure that the full algebraic system
(including constrained degrees of freedom) has full rank.  The primary
modifications here are in threed_dot_mgt.f, but also see changes in
nimcore.

  CRS, 1/4/16

  The 1/4 modifications for boundary conditions have been ported into
the regularity routines.  The primary changes are in threed_dot_mgt.f,
where calls to dirichlet_rhs and the new regularity_pre_feop routines
are coordinated.  Also, finite_element matrix_create routines provide
a scaling factor to the regular_op routines, so that a unique factor
can be used for both boundary and regularity conditions in the 3d
operations.
  Separately, a bug in the quad-point saves for the 3D semi-implicit
operator has been fixed.  The 3D operator needs the qpres storage
updated even when p_computation is "at quads" so there are extra
p_from_nt("all") calls.

  CRS, 1/6/16

  Another bug is now fixed in the new 3D semi-implicit operator.  The
code for gyroviscosity zeroed real_pten if par_visc was not positive,
but that was a hold-over from v_aniso_dot and is not correct for the
v_3dsi_dot. 
  There are also modifications for optimization.  The kinematic and
isotropic viscosity computations have been moved into the real-space
looping for the si operator.  This avoids an extra loop and an extra
fft.
  The real-space ti+ti_eq data is also being saved in new quad-point
storage outside the iterative solve to avoid an extra fft when
gyroviscosity is used.

  CRS, 1/19/16

  The integrands.f file has been separated in terms pieces,
integrands_rhs.f, integrands_dot.f, and integrands_mat.f.  The
integrands.f file, itself, now just holds the module that 
collects the three separate modules.  This has been done only
to reduce compiler-optimization time.  There are no computational
changes.

  CRS, 9/20/19

  There are also corrections for all vector equations in
threed_dot_mgt.f.  They need to call regular_zero_vec on oper
to fully reset the oper vector after the finite-element dot-product
computation.  Although asymmetric solves are not affected, symmetric
solves are.

  CRS, 12/15/16

  Primary changes here impact the polynomial extrapolation for 3D
solves.  Previously, the respective management routines have saved
and extrapolated the change in a field, but that is not appropriate
when the timestep, itself, changes.  Now, the solution at the end
of an advance is saved and extrapolated, and the delta-field for the
guess is created after extrapolating to the end of the advance.

  CRS, 12/28/16

 The no-dealiase option has been changed so that only n up to nphi/2-1
are used.  This eliminates the need for the zero_last_phi calls and
means that the number of Fourier components is a power of 2.
 There is a new fourier_damp option that allows damping of a specified
range of components.  It is intended for no-dealiase cases, but it can
be used with dealiasing.

  CRS, 1/04/17

  nimuw3_4_13

  Kyle Bunkers fixed an error in matrix_init: deallocation of mass_mat
needs to be outside the loop for j_mat structures.

  CRS, 6/06/17

  The second n_store operation in advance_pc (the nonlinear
predictor-corrector algorithm) passed 'ave' into the nchoice parameter.
This should have been 'average', because 'ave' does not flag one
of the select-case options.  This bug appears to date from 7/3/07.
Before then the second n_store option passed 'end' into nchoice.

  CRS, 8/06/17

  Options to set minimum or "floor" values directly on the values
of density and temperature at the nodal locations of the spectral-
element/Fourier expansions have been added.  This is only used
in nonlinear computations.  Unlike diffusion, this approach is
not conservative and should only be used to control the density
and/or internal energy where they are negligibly small, such
as artificial cold-plasma regions.

  CRS, 8/27/17

  This merges the modified boundary module from lsqfem_3411 that
accepts systems with combinations of 3-vectors and scalars.  The
advance-management, three-D dot, and initialization routines now pass
different character strings to direct the boundary module routines.

  CRS, 1/04/18

  The 3D preconditioner is being streamlined, prior to new development.
The limited Fourier off-diagonal coupling is being removed, so the
cpop matrix-integrand routines and their data structures are no longer
needed.

  CRS, 11/18/18

  New timers for matrix-vector multiplications and for quadrature-point
routines have been added.  Also, I/O operations have been changed to
avoid slowing computations needlessly.  The new hist_flush input
controls how frequently nimrod.out, energy.bin, discharge.bin, and
nimhist.bin are opened and closed.  In addition, many of the writes
to standard out have been removed.  Checking on progress of a running
job now requires "tail -f nimrod.out" but that file won't get updated
every step unless hist_flush="always".  Finally, standard-out writes from
the solvers are no longer turned-on by default.

  CRS, 1/3/19

  Integrand and quadrature-point routines have been modified for the
changes in data structures.  When declaring local pointers for arrays,
the contiguous attribution is used.
  Also, short loops in the integrand_dot routines have been manually
unrolled for optimization.

 CRS, 1/6/19

  Preconditioners for 3D solves are now called through external
subroutines, whose names are passed into the 3D solver.  These
subroutines are in the new threed_precon.f file.

  CRS, 1/27/19

  Heating terms associated with hyper-resistive dissipation and hyper-
diffusion for div(B) have been added.  This heating is only applied
in nonlinear computations, where ohm_heat is true.  The boundary
conditions on the auxiliary fields for these hyper-diffusion terms
has also been changed to set all components to zero to avoid flux of
energy across the surfaces.

  CRS, 2/03/19

  The threed_precon routines now need to copy res into zee, because
iter_precon_c3dto2d not longer uses res.  This adds flexibility.

  CRS, 7/09/19

  The total-pressure computations for V-stabilization (poly_divv) now
limits the plasma pressure values to non-negative numbers to avoid
taking the root of a negative.  When this happened in at least some
computations, the resulting NaNs caused the GMRES solver to go to 
maxits without crashing the code.
  There are also corrections so that the energy-correction scheme
runs with the old "precor" algorithm, although it does not enforce
energy conservation as well as with the implicit leapfrog.

  CRS, 12/27/19

  A time-split implicit hyper-viscosity is being added.  The differential
operator is the composite of two Laplacians.  The system solves for
the necessary auxiliary field by packing separate real vector-component
sets into a complex 3-vector system.
  One needs to be careful when using this operator.  It is intended to
provide mesh-scale smoothing, and viscous heating from it is not
available, at least not at this time.  Moreover, there can be significant
time-splitting errors at large dt.  These errors change suddenly if
the time-step changes suddenly, and that can affect the evolution in
unexpected ways.
  Note that the definition of delta2 is also being changed.  It is now
more like the inverse of max(k**2).

  CRS, 1/08/20
